{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:25:39,725 - __main__[line:907] - INFO: step 1 相关配置\n",
      "2020-12-18 11:25:39,727 - __main__[line:951] - INFO: step 2 开始读取数据集\n",
      "2020-12-18 11:25:39,727 - __main__[line:953] - INFO: 读取样本&特征数据集：apply_no|apply_time|target为样本数据，其他为特征数据\n",
      "2020-12-18 11:25:40,364 - __main__[line:964] - INFO: 特征的个数：1103\n",
      "2020-12-18 11:25:40,365 - __main__[line:986] - INFO: 删除特征全为空的样本量\n",
      "2020-12-18 11:25:40,436 - __main__[line:991] - INFO: 样本数据集情况：\n",
      "2020-12-18 11:25:40,438 - __main__[line:992] - INFO: 0    4623\n",
      "1     595\n",
      "Name: target, dtype: int64\n",
      "2020-12-18 11:25:40,439 - __main__[line:995] - INFO: EDA，整体数据探索性数据分析\n",
      "2020-12-18 11:25:40,440 - __main__[line:1001] - INFO: step 3 划分训练集和测试集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除特征全为空的样本之前的数据集行列： (5218, 1105)\n",
      "删除特征全为空的样本之后的数据集行列： (5218, 1105)\n",
      "样本y值在0，1\n",
      "样本情况： (5218, 3)\n",
      "分布情况： target\n",
      "0    4623\n",
      "1     595\n",
      "Name: apply_no, dtype: int64\n",
      "样本drop_duplicates情况： (5218, 3)\n",
      "样本y值在0，1的样本情况： (5218, 3)\n",
      "              count      mean       std  min  25%  50%  75%  max\n",
      "apply_month                                                     \n",
      "2019/1/       515.0  0.102913  0.304140  0.0  0.0  0.0  0.0  1.0\n",
      "2019/2/      1241.0  0.089444  0.285499  0.0  0.0  0.0  0.0  1.0\n",
      "2019/3/      1989.0  0.120161  0.325231  0.0  0.0  0.0  0.0  1.0\n",
      "2019/4/       588.0  0.141156  0.348479  0.0  0.0  0.0  0.0  1.0\n",
      "2019/5/       885.0  0.123164  0.328811  0.0  0.0  0.0  0.0  1.0\n",
      "        count      mean       std  min  25%  50%  75%  max\n",
      "type                                                      \n",
      "test   1305.0  0.114176  0.318147  0.0  0.0  0.0  0.0  1.0\n",
      "train  3913.0  0.113979  0.317826  0.0  0.0  0.0  0.0  1.0\n",
      "train    3913\n",
      "test     1305\n",
      "Name: type, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:25:40,755 - __main__[line:1029] - INFO: 分开训练集和测试集为两个df\n",
      "2020-12-18 11:25:40,810 - __main__[line:1033] - INFO: EDA，训练集探索性数据分析\n",
      "2020-12-18 11:25:40,811 - __main__[line:1040] - INFO: step 4 变量初筛\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除缺失率前变量数量： 1103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:25:41,118 - __main__[line:1050] - INFO: step 5 数据woe处理\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除缺失率后变量数量： 1020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:25:42,711 - __main__[line:1088] - INFO: step 5.1 类别变量数据处理\n",
      "2020-12-18 11:25:43,672 - __main__[line:1110] - INFO: ============入模数据不需要转化为woe值===========\n",
      "2020-12-18 11:25:43,793 - __main__[line:1186] - INFO: step 6 开始训练模型\n",
      "2020-12-18 11:25:43,794 - __main__[line:1189] - INFO: step 6.1 ===筛选变量===\n",
      "2020-12-18 11:25:43,795 - __main__[line:1193] - INFO: step 6.1 ===筛选变量===10折交叉后，计算变量的平均重要性\n",
      "2020-12-18 11:25:43,797 - __main__[line:1195] - INFO: 筛选前数据集大小：(3913, 1020)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体数据集大小： (5218, 1020)\n",
      "训练集大小： (3913, 1020)\n",
      "测试集大小： (1305, 1020)\n",
      "无oot数据集\n",
      "subsample is 0.3 and colsample_bytree is 0.3 model result is : \n",
      "xgb train auc is ： 0.8604342545538266\n",
      "xgb train ks is ： 0.5649286482025917\n",
      "xgb test auc is ： 0.5363321799307958\n",
      "xgb test ks is ： 0.09173614175239775\n",
      "subsample is 0.4 and colsample_bytree is 0.4 model result is : \n",
      "xgb train auc is ： 0.9358668082536045\n",
      "xgb train ks is ： 0.7371449709690729\n",
      "xgb test auc is ： 0.524958779405959\n",
      "xgb test ks is ： 0.06336360047374645\n",
      "subsample is 0.5 and colsample_bytree is 0.5 model result is : \n",
      "xgb train auc is ： 0.9696633602408875\n",
      "xgb train ks is ： 0.8167035508400149\n",
      "xgb test auc is ： 0.5329184180581036\n",
      "xgb test ks is ： 0.11533057755277398\n",
      "subsample is 0.6 and colsample_bytree is 0.6 model result is : \n",
      "xgb train auc is ： 0.9859857386944941\n",
      "xgb train ks is ： 0.8899437489410081\n",
      "xgb test auc is ： 0.5235247671907295\n",
      "xgb test ks is ： 0.06587167042103059\n",
      "subsample is 0.7 and colsample_bytree is 0.7 model result is : \n",
      "xgb train auc is ： 0.996322792349649\n",
      "xgb train ks is ： 0.9466507402918743\n",
      "xgb test auc is ： 0.5706381644643646\n",
      "xgb test ks is ： 0.1256299203455563\n",
      "subsample is 0.8 and colsample_bytree is 0.8 model result is : \n",
      "xgb train auc is ： 0.9966571427462778\n",
      "xgb train ks is ： 0.9546344069193071\n",
      "xgb test auc is ： 0.544872390329997\n",
      "xgb test ks is ： 0.12971133972736348\n",
      "subsample is 0.9 and colsample_bytree is 0.9 model result is : \n",
      "xgb train auc is ： 0.9983877455729291\n",
      "xgb train ks is ： 0.9623839635978431\n",
      "xgb test auc is ： 0.5488841411021573\n",
      "xgb test ks is ： 0.12258191867350965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:27:17,250 - __main__[line:1199] - INFO: 将特征重要性持久化\n",
      "2020-12-18 11:27:17,253 - __main__[line:1203] - INFO: 根据10折拟合模型处理后的变量重要性进行变量相关性筛选\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample is 1.0 and colsample_bytree is 1.0 model result is : \n",
      "xgb train auc is ： 0.9979751429558127\n",
      "xgb train ks is ： 0.961937731927294\n",
      "xgb test auc is ： 0.5336151041545715\n",
      "xgb test ks is ： 0.10213998745965025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:27:17,848 - __main__[line:1206] - INFO: 筛选后数据集大小：(3913, 728)\n",
      "2020-12-18 11:27:17,849 - __main__[line:1211] - INFO: 最终入模变量的数量：728\n",
      "2020-12-18 11:27:17,851 - __main__[line:1212] - INFO: 最终入模变量：['td_lend_muti_platform_num_m300', 'td_idcard_lend_vertivcal_electricity_num_d90', 'td_idcard_format', 'td_idcard_lend_online_bank_num_d90', 'td_email_loan_muti_platform_d90', 'td_mobile_loan_gurantee_num_d30', 'td_email_lend_traffic_lease_num_d90', 'td_lend_vertivcal_electricity_num_d90', 'td_loan_level1_third_service_d90', 'tzre_es_m3_overdue_repay_sum', 'td_idcard_loan_person_insurance_num_d7', 'td_idcard_lend_finance_lease_num_d90', 'td_lend_crowd_funding_num_d90', 'td_idcard_loan_car_lease_num_d90', 'td_mobile_loan_third_pay_num_d30', 'td_email_lend_credit_card_num_d90', 'td_lend_credit_card_num_d90', 'td_mobile_loan_device_lease_num_d90', 'td_email_lend_house_finance_num_d90', 'td_loan_level1_lease_d7', 'td_mobile_loan_car_lease_num_d7', 'xy_black_fee', 'td_loan_car_lease_num_d30', 'td_ip_over6_and_device_below60_d1', 'tzre_rf_m1_max_loan_offer_amount_level', 'td_device_over3_and_blow10_h1', 'td_idcard_address', 'td_device_idcard_mobile_over3_d7', 'td_idcard_conn_add_d90', 'td_idcard_loan_third_pay_num_d90', 'td_device_idcard_mobile_over5_d7', 'td_idcard_or_mobile_over5_d1_below3_h1', 'td_loan_finance_lease_num_d30', 'td_idcard_lend_bank_person_small_num_d90', 'td_add_credit_overdue', 'td_idcard_loan_house_lease_num_d30', 'td_second_contact_crime_other', 'td_mobile_loan_big_data_finance_num_d7', 'td_email_loan_online_bank_num_d90', 'td_mobile_loan_finance_institution_num_d7', 'td_email_loan_house_lease_num_d90', 'td_fifth_contact_court_closed_other', 'td_email_loan_big_consume_finance_num_d30', 'tzre_product_id', 'td_mobile_lend_house_finance_num_d90', 'td_loan_car_lease_num_d7', 'td_email_loan_inter_finance_num_d7', 'tzre_rf_m12_ave_overdue_repay_amount_level', 'td_email_loan_bank_person_small_num_d30', 'td_lend_compre_electricity_num_d90', 'sd_loaninfo_pid_all_overduetenantcount', 'sd_loaninfo_pid_d360_daysfromlastloan', 'td_lend_car_lease_num_d90', 'td_ip_conn_idcard_or_mobile_over5_d1_below3_h1', 'td_email_loan_muti_platform_d30_num', 'td_mobile_lend_asset_transfer_num_d90', 'td_idcard_lend_compre_electricity_num_d90', 'td_idcard_lend_asset_transfer_num_d90', 'ten_fraud_abnormal_pay', 'td_loan_person_insurance_num_d30', 'td_email_loan_big_data_finance_num_d30', 'td_email_loan_finance_institution_num_d7', 'td_ip_over3_and_blow10_h1', 'td_email_lend_small_amount_num_d90', 'td_mobile_lend_gurantee_num_d90', 'td_idcard_loan_asset_transfer_num_d7', 'sd_loaninfo_pid_d90_maxoverduedays', 'td_loan_level1_third_pay_d30', 'td_qq_high_risk', 'td_idcard_lend_person_insurance_num_d90', 'td_mobile_refuse_over4_d30', 'td_device_conn_idcard_num_d7', 'td_idcard_court_closed', 'td_lend_house_finance_num_d90', 'tzre_rf_m6_ave_overdue_repay_amount_level', 'td_mobile_company_person_owe', 'td_fifth_contact_is_family', 'td_email_loan_bank_finance_num_d90', 'td_third_contact_court_closed_family', 'td_loan_level1_third_pay_d7', 'td_lend_asset_transfer_num_d90', 'td_email_loan_house_lease_num_d30', 'td_email_loan_property_insurance_num_d90', 'td_promise_not_result', 'sd_loaninfo_mobile_d90_maxloanperioddays', 'td_idcard_student_lend_owe', 'td_email_loan_traffic_lease_num_d90', 'td_idcard_credit_overdue', 'td_first_contact_is_family', 'td_email_high_risk', 'td_mobile_num_d30', 'td_loan_muti_platform_d7', 'td_lend_self_num_d90', 'td_loan_other_d90', 'td_tele_medium_risk', 'sd_blacklist_pid_last6mquerycount', 'ten_fraud_record_found', 'td_mobile_loan_bank_public_num_d30', 'td_mobile_loan_third_service_num_d7', 'td_fifth_contact_court_excute_other', 'td_mobile_loan_device_lease_num_d30', 'td_loan_online_bank_num_d7', 'td_mobile_lend_car_finance_num_d90', 'td_lend_finance_lease_num_d90', 'td_mobile_car_rent_break', 'td_idcard_court_closed_blur', 'td_loan_third_service_num_d7', 'td_email_loan_p2p_num_d7', 'td_idcard_lend_small_amount_num_d90', 'td_idcard_credit_overdue_num', 'sd_loaninfo_pid_d90_overdueloancount', 'tzre_rf_m1_ave_repay_amount_level', 'td_mobile_lend_house_lease_num_d90', 'td_loan_direct_bank_num_d7', 'td_mobile_lend_small_amount_num_d90', 'td_loan_finance_institution_num_d30', 'td_first_contact_mobile_false_family', 'td_second_contact_mobile_false_family', 'td_email_loan_online_bank_num_d30', 'td_lend_bank_person_num_d90', 'td_idcard_crime', 'sd_loaninfo_mobile_d360_daysfromlastloan', 'sd_loaninfo_mobile_d360_monthsfromfirstloan', 'td_lend_muti_platform_over100_history', 'td_lend_small_amount_num_d90', 'td_email_loan_inter_finance_num_d90', 'td_idcard_company_person_taxes', 'td_email_loan_small_amount_num_d90', 'td_idcard_loan_asset_transfer_num_d30', 'td_lend_bank_finance_num_d90', 'tzre_pi_m24_overdue_replay_count', 'sd_loaninfo_mobile_d360_maxoverduedays', 'td_email_loan_device_lease_num_d30', 'td_qq_credit_overdue_num', 'sd_blacklist_pid_lastconfirmatdays', 'td_mobile_loan_direct_bank_num_d7', 'td_fifth_contact_court_excute_family', 'td_idcard_loan_finance_institution_num_d30', 'td_mobile_loan_online_bank_num_d7', 'td_email_loan_gurantee_num_d30', 'td_idcard_conn_mother_mobile_over2_d90', 'td_tele_credit_overdue_num', 'td_mobile_loan_asset_transfer_num_d7', 'td_loan_car_lease_num_d90', 'td_email_loan_bank_public_num_d90', 'td_lend_level1_raise_d90', 'td_email_loan_finance_institution_num_d30', 'sd_loaninfo_mobile_d360_overduetenantcount', 'td_loan_crowd_funding_num_d90', 'td_email_loan_credit_card_num_d7', 'sd_loaninfo_pid_d90_maxloanperioddays', 'td_mobile_lend_third_service_num_d90', 'td_email_loan_asset_transfer_num_d7', 'td_lend_online_bank_num_d90', 'td_idcard_num_d7', 'td_email_loan_muti_platform_d7_num', 'td_idcard_device_over10_d30', 'tzre_rf_m6_ave_apply_request_amount_level', 'td_idcard_lend_property_insurance_num_d90', 'td_idcard_loan_vertivcal_electricity_num_d90', 'td_mobile_loan_car_lease_num_d30', 'td_email_loan_bank_finance_num_d7', 'td_lend_bank_person_small_num_d90', 'td_idcard_loan_crowd_funding_num_d90', 'td_email_loan_credit_card_num_d90', 'td_second_contact_court_excute_family', 'sd_loaninfo_mobile_d90_overduetenantcount', 'td_loan_asset_transfer_num_d7', 'td_email_lend_vertivcal_electricity_num_d90', 'td_device_idcard_mobile_over3_d1', 'td_idcard_loan_car_finance_num_d30', 'td_mobile_loan_car_finance_num_d30', 'td_device_num_d30', 'td_mobile_not_trueip_add', 'td_idcard_lend_third_pay_num_d90', 'td_mobile_false_blur', 'td_loan_traffic_lease_num_d30', 'td_idcard_loan_house_lease_num_d90', 'td_email_loan_inter_finance_num_d30', 'td_idcard_credit_overdue_blur', 'td_mobile_loan_finance_institution_num_d90', 'td_idcard_lend_bank_person_num_d90', 'td_mobile_credit_overdue_repayment', 'tzre_rf_m12_ave_overdue_delay_level', 'td_tele_low_risk', 'td_tele_high_risk', 'tzre_rf_m3_ave_overdue_delay_level', 'td_loan_gurantee_num_d30', 'td_company_name_over12_d90', 'td_idcard_second', 'td_loan_house_lease_num_d7', 'td_fifth_contact_is_other', 'td_idcard_taxes', 'tzre_rf_m3_ave_overdue_amount_level', 'td_idcard_loan_direct_bank_num_d30', 'td_email_loan_third_pay_num_d30', 'td_email_loan_third_pay_num_d7', 'tzre_rf_m3_ave_overdue_repay_delay_level', 'tzre_pi_m1_register_count', 'td_idcard_conn_partner_idcard_over2_d90', 'td_mobile_lend_third_pay_num_d90', 'td_email_lend_compre_electricity_num_d90', 'td_idcard_loan_bank_person_small_num_d7', 'td_bank_card_name_conn_idacrd_num_d90', 'td_email_lend_car_finance_num_d90', 'td_idcard_car_rent_break', 'td_lend_inter_finance_num_d90', 'td_email_loan_direct_bank_num_d90', 'td_mobile_loan_third_service_num_d30', 'td_fourth_contact_mobile_false_family', 'sd_loaninfo_pid_d90_overduetenantcount', 'td_mobile_high_risk', 'td_mobile_loan_muti_platform_d90', 'td_email_lend_crowd_funding_num_d90', 'td_email_loan_muti_platform_d90_num', 'td_idcard_loan_person_insurance_num_d90', 'td_lend_person_insurance_num_d90', 'td_email_loan_car_finance_num_d30', 'sd_loaninfo_mobile_d90_maxoverduedays', 'sd_loaninfo_mobile_d360_overdueloancount', 'td_device_conn_idcard_num_d1', 'td_mobile_loan_person_insurance_num_d90', 'td_email_loan_house_lease_num_d7', 'td_idcard_company_person_owe', 'td_email_lend_device_lease_num_d90', 'tzre_rf_m12_ave_apply_request_amount_level', 'tzre_rf_m3_ave_loan_offer_amount_level', 'sd_loaninfo_mobile_all_overdueloancount', 'td_loan_house_finance_num_d90', 'td_third_contact_court_excute_family', 'sd_loaninfo_mobile_d360_monthsfromlastoverdue', 'td_idcard_lend_car_finance_num_d90', 'td_idcard_lend_crowd_funding_num_d90', 'td_loan_level1_third_service_d7', 'td_email_loan_asset_transfer_num_d30', 'td_idcard_over_device_num_d1', 'td_idcard_loan_third_pay_num_d7', 'td_tele_credit_overdue', 'td_mobile_loan_asset_transfer_num_d90', 'td_age_risk', 'td_third_contact_crime_family', 'td_mobile_fraud', 'sd_loaninfo_pid_d360_loancount', 'td_idcard_court_excute', 'td_idcard_loan_car_finance_num_d90', 'td_mobile_credit_overdue', 'td_fourth_contact_court_closed_family', 'td_idcard_conn_father_idcard_over2_d90', 'td_company_name_over10_d30', 'tzre_rf_m12_ave_overdue_amount_level', 'td_loan_asset_transfer_num_d30', 'td_mobile_loan_crowd_funding_num_d90', 'td_idcard_lend_traffic_lease_num_d90', 'td_idcard_loan_bank_person_num_d7', 'td_mobile_lend_vertivcal_electricity_num_d90', 'sd_loaninfo_pid_d360_overduetenantcount', 'td_mobile_lend_finance_lease_num_d90', 'td_lend_third_pay_num_d90', 'td_idcard_loan_device_lease_num_d90', 'sd_loaninfo_pid_d360_overduefor2termtenantcount', 'td_idcard_loan_vertivcal_electricity_num_d7', 'td_lend_level1_electricity_d90', 'td_loan_vertivcal_electricity_num_d7', 'sd_blacklist_pid_blacklevel', 'td_loan_device_lease_num_d30', 'td_mobile_loan_third_pay_num_d7', 'td_email_lend_consume_stage_num_d90', 'td_lend_traffic_lease_num_d90', 'td_email_lend_person_insurance_num_d90', 'td_loan_device_lease_num_d90', 'td_mobile_lend_car_lease_num_d90', 'td_idcard_lend_house_finance_num_d90', 'td_idcard_loan_traffic_lease_num_d7', 'td_loan_third_pay_num_d7', 'td_mobile_loan_person_insurance_num_d30', 'td_idcard_lend_car_lease_num_d90', 'td_qq_medium_risk', 'td_email_loan_car_lease_num_d90', 'td_mobile_loan_crowd_funding_num_d30', 'td_lend_third_service_num_d90', 'td_device_vpn_over2_d1', 'td_loan_property_insurance_num_d7', 'td_email_loan_third_service_num_d90', 'td_loan_crowd_funding_num_d30', 'td_email_loan_small_amount_num_d30', 'td_loan_level1_finance_institution_d90', 'td_email_loan_consume_stage_num_d30', 'td_email_lend_car_lease_num_d90', 'td_ip_conn_idcard_or_mobile_over3_h1', 'td_lend_big_data_finance_num_d90', 'td_loan_level1_raise_d7', 'td_fifth_contact_court_dishonesty_family', 'td_third_contact_is_other', 'td_loan_vertivcal_electricity_num_d90', 'td_lend_level1_online_finance_d90', 'td_idcard_loan_direct_bank_num_d7', 'td_idcard_loan_car_lease_num_d30', 'td_email_low_risk', 'td_second_contact_crime_family', 'td_loan_level1_raise_d30', 'td_email_loan_crowd_funding_num_d7', 'td_email_loan_traffic_lease_num_d30', 'td_idcard_loan_muti_platform_d7', 'td_first_contact_court_closed_other', 'td_black_num', 'td_idcard_mobile_conn_device_over3_h1', 'sd_blacklist_mobile_lastconfirmatdays', 'td_mobile_conn_idcard_d90', 'td_idcard_loan_bank_public_num_d90', 'td_idcard_court_dishonesty_blur', 'td_loan_traffic_lease_num_d90', 'td_third_contact_mobile_false_other', 'td_email_loan_house_finance_num_d30', 'sd_loaninfo_pid_d90_overduefor2termtenantcount', 'td_loan_muti_platform_d90', 'tzre_rf_m12_ave_overdue_repay_delay_level', 'td_mobile_loan_finance_lease_num_d30', 'td_idcard_conn_partner_mobile_over2_d90', 'td_email_lend_asset_transfer_num_d90', 'td_mobile_loan_crowd_funding_num_d7', 'td_email_loan_consume_stage_num_d7', 'sd_loaninfo_pid_all_maxoverduedays', 'sd_loaninfo_pid_all_loancount', 'td_mobile_lend_direct_bank_num_d90', 'td_idcard_loan_device_lease_num_d30', 'td_idcard_lend_credit_card_num_d90', 'tzre_es_m1_repay_fail_sum', 'td_mobile_loan_car_finance_num_d7', 'sd_loaninfo_pid_d360_overdueloancount', 'td_mobile_loan_traffic_lease_num_d90', 'td_email_loan_asset_transfer_num_d90', 'td_idcard_loan_gurantee_num_d7', 'td_mobile_lend_bank_finance_num_d90', 'applthst_periods', 'td_lend_level1_lease_d90', 'td_loan_bank_public_num_d90', 'td_loan_gurantee_num_d7', 'td_ip_vpn_over2_d1', 'sd_loaninfo_mobile_d90_overdueloancount', 'td_email_loan_third_service_num_d30', 'td_third_contact_court_dishonesty_family', 'td_ip_http_over2_d1', 'td_idcard_loan_house_finance_num_d30', 'td_mobile_loan_car_lease_num_d90', 'td_first_contact_court_excute_other', 'td_mobile_loan_credit_card_num_d7', 'td_email_loan_online_bank_num_d7', 'td_second_contact_mobile_false_other', 'td_loan_compre_electricity_num_d7', 'td_tele_fraud', 'td_loan_other_d30', 'xy_black_desc', 'td_idcard_loan_finance_institution_num_d90', 'td_black_score', 'td_email_loan_finance_lease_num_d30', 'td_lend_level1_third_service_d90', 'td_idcard_loan_person_insurance_num_d30', 'sd_loaninfo_mobile_d360_overduefor2termtenantcount', 'td_idcard_loan_bank_public_num_d30', 'td_mobile_lend_property_insurance_num_d90', 'td_email_loan_person_insurance_num_d7', 'td_lend_direct_bank_num_d90', 'td_loan_gurantee_num_d90', 'td_device_idcard_mobile_over3_d30', 'td_third_contact_court_closed_other', 'td_email_loan_finance_lease_num_d90', 'td_email_lend_bank_person_num_d90', 'td_email_loan_big_data_finance_num_d90', 'sd_loaninfo_pid_d360_monthsfornormalrepay', 'td_email_medium_risk', 'sd_loaninfo_mobile_all_loancount', 'td_edu_age_job_risk', 'td_mobile_loan_car_finance_num_d90', 'td_loan_finance_lease_num_d7', 'td_second_contact_is_family', 'td_loan_other_d7', 'td_mobile_false', 'sd_loaninfo_mobile_d360_loantenantcount', 'td_email_lend_big_data_finance_num_d90', 'sd_loaninfo_pid_d360_loantenantcount', 'td_idcard_or_mobile_over3_h1', 'td_device_conn_idcard_or_mobile_over3_h1', 'td_idcard_lend_big_data_finance_num_d90', 'td_email_credit_overdue_num', 'td_idcard_loan_bank_finance_num_d7', 'td_second_contact_is_other', 'td_lend_big_consume_finance_num_d90', 'td_email_lend_inter_finance_num_d90', 'td_mobile_format', 'td_mobile_loan_asset_transfer_num_d30', 'td_email_lend_bank_public_num_d90', 'td_first_contact_mobile_false_other', 'td_idcard_over_device_num_d30', 'td_email_loan_p2p_num_d90', 'sd_loaninfo_pid_d360_maxloanperioddays', 'sd_loaninfo_mobile_all_overduetenantcount', 'td_device_socks_over2_d1', 'td_ip_socks_over2_d1', 'td_idcard_loan_finance_lease_num_d7', 'td_lend_property_insurance_num_d90', 'td_first_contact_is_other', 'tzre_es_m1_overdue_repay_sum', 'td_first_contact_court_dishonesty_family', 'td_third_contact_crime_other', 'td_fourth_contact_court_excute_family', 'td_loan_house_finance_num_d30', 'td_email_loan_vertivcal_electricity_num_d7', 'td_mobile_loan_gurantee_num_d7', 'td_idcard_loan_muti_platform_d30', 'td_mobile_credit_overdue_num', 'td_idcard_loan_finance_lease_num_d90', 'td_idcard_loan_third_service_num_d7', 'td_fifth_contact_court_dishonesty_other', 'td_qq_credit_overdue_repayment', 'td_email_loan_finance_lease_num_d7', 'td_mobile_lend_credit_card_num_d90', 'td_fourth_contact_crime_other', 'td_email_lend_muti_platform_d90_plat_num', 'td_mobile_loan_bank_public_num_d90', 'td_mobile_loan_house_lease_num_d30', 'td_loan_big_data_finance_num_d30', 'td_third_contact_court_dishonesty_other', 'td_device_id_abnormal', 'td_mobile_loan_big_data_finance_num_d30', 'td_lend_other_d90', 'td_loan_level1_raise_d90', 'td_loan_house_finance_num_d7', 'td_loan_bank_public_num_d7', 'td_idcard_loan_traffic_lease_num_d30', 'td_mobile_loan_vertivcal_electricity_num_d90', 'td_email_loan_house_finance_num_d7', 'td_lend_conn_idcard_d90', 'td_idcard_lend_house_lease_num_d90', 'td_mobile_lend_bank_person_small_num_d90', 'td_idcard_loan_finance_lease_num_d30', 'td_email_loan_car_lease_num_d30', 'td_company_agency_blur', 'td_email_loan_device_lease_num_d7', 'td_device_conn_idcard_or_mobile_over5_d1_below3_h1', 'td_email_loan_p2p_num_d30', 'td_idcard_num_d30', 'td_second_contact_court_closed_other', 'td_idcard_high_risk', 'td_second_contact_court_dishonesty_other', 'td_mobile_loan_house_finance_num_d30', 'td_dcard_mobile_conn_device_over5_d1_below3_h1', 'td_loan_traffic_lease_num_d7', 'td_idcard_conn_email_d90', 'td_email_loan_bank_person_small_num_d7', 'td_time_between_1_and_5', 'sd_blacklist_mobile_blacklevel', 'td_loan_car_finance_num_d7', 'td_idcard_loan_car_finance_num_d7', 'td_ip_not_tureip_city', 'td_email_loan_big_data_finance_num_d7', 'td_idcard_loan_vertivcal_electricity_num_d30', 'td_mobile_lend_big_consume_finance_num_d90', 'td_idcard_lend_muti_platform_d90_plat_num', 'td_mobile_loan_muti_platform_d7', 'td_idcard_loan_bank_person_num_d30', 'td_email_loan_device_lease_num_d90', 'td_idcard_device_over5_d7', 'td_email_lend_house_lease_num_d90', 'td_idcard_loan_credit_card_num_d7', 'td_email_loan_big_consume_finance_num_d90', 'td_lend_finance_institution_num_d90', 'td_lend_car_finance_num_d90', 'sd_loaninfo_mobile_d90_loancount', 'td_device_proxy', 'td_idcard_loan_property_insurance_num_d7', 'td_first_contact_court_closed_family', 'td_email_loan_third_service_num_d7', 'td_loan_person_insurance_num_d7', 'td_idcard_loan_online_bank_num_d7', 'td_email_loan_big_consume_finance_num_d7', 'sd_loaninfo_pid_all_remainingoverdueamount', 'td_email_loan_gurantee_num_d7', 'td_loan_level1_finance_institution_d30', 'td_idcard_lend_finance_institution_num_d90', 'td_idcard_loan_crowd_funding_num_d30', 'td_loan_house_lease_num_d30', 'tzre_rf_m12_max_loan_offer_amount_level', 'td_idcard_loan_third_service_num_d90', 'td_idcard_lend_inter_finance_num_d90', 'td_lend_level1_third_pay_d90', 'td_qq_credit_overdue', 'td_loan_level1_finance_institution_d7', 'tzre_rf_m1_max_apply_request_amount_level', 'applthst_periods_type', 'td_loan_bank_public_num_d30', 'td_first_contact_court_excute_family', 'td_mobile_lend_consume_stage_num_d90', 'td_fourth_contact_court_dishonesty_family', 'td_lend_house_lease_num_d90', 'td_device_conn_mobile_num_d7', 'td_third_contact_mobile_false_family', 'td_loan_level1_third_pay_d90', 'sd_loaninfo_mobile_all_maxoverduedays', 'sd_blacklist_mobile_last6mquerycount', 'td_loan_bank_person_num_d30', 'td_email_loan_person_insurance_num_d30', 'td_mobile_loan_muti_platform_d30', 'td_mobile_loan_finance_lease_num_d7', 'td_fifth_contact_crime_family', 'td_fourth_contact_court_closed_other', 'td_mobile_small', 'td_idcard_loan_crowd_funding_num_d7', 'sd_loaninfo_pid_all_overdueloancount', 'applthst_repay_type', 'td_email_loan_person_insurance_num_d90', 'td_idcard_lend_bank_public_num_d90', 'td_idcard_court_dishonesty', 'applthst_loan_rate', 'td_mobile_loan_vertivcal_electricity_num_d7', 'td_idcard_loan_house_finance_num_d7', 'td_email_lend_finance_institution_num_d90', 'td_fifth_contact_crime_other', 'td_idcard_court_excute_blur', 'td_lend_muti_platform_d90', 'td_third_contact_is_family', 'ten_fraud_idcard_found', 'td_idcard_loan_device_lease_num_d7', 'td_idcard_credit_overdue_repayment', 'td_fourth_contact_mobile_false_other', 'td_email_credit_overdue_repayment', 'td_email_lend_muti_platform_d90_num', 'td_loan_bank_person_small_num_d7', 'td_email_loan_direct_bank_num_d7', 'tzre_rf_m6_max_overdue_repay_delay_level', 'td_idcard_conn_mother_idcard_over2_d90', 'td_idcard_lend_big_consume_finance_num_d90', 'td_idcard_lend_p2p_num_d90', 'td_idcard_lend_third_service_num_d90', 'td_mobile_loan_house_finance_num_d7', 'td_mobile_loan_property_insurance_num_d7', 'td_loan_third_service_num_d30', 'td_add_move_false', 'td_idcard_loan_traffic_lease_num_d90', 'td_loan_car_finance_num_d30', 'td_email_loan_third_pay_num_d90', 'td_idcard_loan_third_pay_num_d30', 'tzre_rf_m3_ave_repay_amount_level', 'td_mobile_loan_house_finance_num_d90', 'td_email_loan_gurantee_num_d90', 'td_mobile_loan_traffic_lease_num_d30', 'td_email_loan_direct_bank_num_d30', 'sd_loaninfo_pid_d90_loancount', 'tzre_rf_m6_max_loan_offer_amount_level', 'td_loan_person_insurance_num_d90', 'td_idcard_device_over3_d1', 'tzre_rf_m6_max_overdue_amount_level', 'td_email_lend_third_pay_num_d90', 'td_email_loan_bank_finance_num_d30', 'td_third_contact_court_excute_other', 'td_email_loan_muti_platform_d7_plat_num', 'td_email_loan_compre_electricity_num_d90', 'td_edu_age_industry_risk', 'td_fourth_contact_is_family', 'td_loan_finance_institution_num_d90', 'td_email_loan_bank_person_num_d30', 'td_email_loan_muti_platform_d7', 'td_idcard_num_over2_d90', 'td_email_loan_bank_person_num_d90', 'td_email_lend_online_bank_num_d90', 'td_email_loan_finance_institution_num_d90', 'td_second_contact_court_closed_family', 'td_loan_level1_lease_d90', 'td_idcard_loan_asset_transfer_num_d90', 'td_idcard_loan_third_service_num_d30', 'td_device_num_d7', 'td_mobile_lend_inter_finance_num_d90', 'td_first_contact_crime_family', 'sd_loaninfo_pid_d360_monthsfromfirstloan', 'td_idcard_loan_inter_finance_num_d7', 'td_mobile_lend_compre_electricity_num_d90', 'td_email_loan_compre_electricity_num_d30', 'td_email_loan_traffic_lease_num_d7', 'td_email_loan_car_finance_num_d7', 'td_idcard_loan_muti_platform_d90', 'td_mobile_loan_traffic_lease_num_d7', 'sd_loaninfo_pid_all_overduecount', 'td_lend_level1_bank_d90', 'td_mobile_loan_bank_public_num_d7', 'td_email_lend_property_insurance_num_d90', 'td_email_lend_third_service_num_d90', 'td_email_loan_bank_person_small_num_d90', 'td_fourth_contact_is_other', 'td_email_lend_gurantee_num_d90', 'td_email_lend_bank_finance_num_d90', 'td_fourth_contact_court_excute_other', 'td_email_loan_crowd_funding_num_d30', 'sd_loaninfo_pid_d90_loantenantcount', 'td_email_lend_finance_lease_num_d90', 'td_idcard_loan_house_finance_num_d90', 'td_loan_vertivcal_electricity_num_d30', 'td_idcard_loan_car_lease_num_d7', 'td_lend_bank_public_num_d90', 'td_idcard_conn_company_add_over2_d90', 'td_email_lend_p2p_num_d90', 'td_mobile_loan_finance_institution_num_d30', 'td_qq_low_risk', 'td_loan_bank_person_num_d7', 'td_email_loan_consume_stage_num_d90', 'td_mobile_loan_third_service_num_d90', 'td_mobile_lend_big_data_finance_num_d90', 'tzre_rf_m6_max_repay_amount_level', 'td_idcard_loan_finance_institution_num_d7', 'td_fifth_contact_mobile_false_other', 'td_first_contact_crime_other', 'td_email_lend_muti_platform_d90', 'td_loan_muti_platform_d30', 'td_mobile_lend_muti_platform_d90', 'xy_black_currently_performance', 'td_company_name_over5_d7', 'td_mobile_lend_device_lease_num_d90', 'sd_loaninfo_mobile_d360_loancount', 'td_loan_device_lease_num_d7', 'tzre_rf_m3_max_overdue_repay_delay_level', 'td_loan_crowd_funding_num_d7', 'td_loan_third_pay_num_d30', 'td_loan_direct_bank_num_d30', 'td_loan_third_pay_num_d90', 'td_fifth_contact_court_closed_family', 'td_mobile_loan_direct_bank_num_d90', 'td_lend_level1_finance_institution_d90', 'td_idcard_over_device_num_d7', 'td_fourth_contact_crime_family', 'td_email_loan_vertivcal_electricity_num_d90', 'td_email_loan_muti_platform_d90_plat_num', 'td_idcard_loan_house_lease_num_d7', 'td_email_loan_property_insurance_num_d30', 'td_second_contact_court_excute_other', 'td_email_loan_muti_platform_d30_plat_num', 'td_email_loan_car_finance_num_d90', 'td_final_score', 'sd_loaninfo_mobile_all_remainingoverdueamount', 'td_loan_level1_lease_d30', 'td_idcard_lend_bank_finance_num_d90', 'td_loan_finance_institution_num_d7', 'td_loan_level1_electricity_d30', 'sd_loaninfo_mobile_d90_loantenantcount', 'td_email_loan_bank_person_num_d7', 'td_idcard_conn_father_mobile_over2_d90', 'td_mobile_lend_bank_person_num_d90', 'sd_loaninfo_mobile_d90_overduefor2termtenantcount', 'tzre_rf_m12_max_overdue_repay_delay_level', 'td_idcard_lend_direct_bank_num_d90', 'td_email_loan_car_lease_num_d7', 'td_idcard_risk_group', 'tzre_pi_m1_overdue_replay_count', 'td_email_loan_credit_card_num_d30', 'td_email_lend_big_consume_finance_num_d90', 'td_email_loan_house_finance_num_d90', 'td_mobile_loan_vertivcal_electricity_num_d30', 'sd_loaninfo_mobile_d360_monthsfornormalrepay', 'td_mobile_loan_house_lease_num_d90', 'td_idcard_loan_gurantee_num_d30', 'td_mobile_num_d7', 'td_device_http_over2_d1', 'td_fourth_contact_court_dishonesty_other', 'sd_loaninfo_pid_d360_maxoverduedays', 'td_email_loan_small_amount_num_d7', 'td_lend_gurantee_num_d90', 'td_email_loan_bank_public_num_d7', 'td_tele_conn_company_over2_d90', 'sd_loaninfo_mobile_d360_maxloanperioddays', 'td_idcard_loan_big_data_finance_num_d7', 'td_email_credit_overdue', 'td_mobile_loan_house_lease_num_d7', 'td_email_loan_crowd_funding_num_d90', 'td_email_format', 'td_email_loan_vertivcal_electricity_num_d30', 'td_mobile_loan_finance_lease_num_d90', 'tzre_rf_m1_ave_overdue_amount_level', 'td_bank_card_name_conn_idacrd_d90', 'tzre_rf_m24_max_repay_amount_level', 'td_mobile_loan_device_lease_num_d7', 'td_ip_device_miss_over5_and_mobile_over2_d1', 'td_mobile_lend_online_bank_num_d90', 'td_first_contact_court_dishonesty_other', 'sd_blacklist_pid_last6mtenantcount', 'td_email_loan_property_insurance_num_d7', 'td_idcard_loan_online_bank_num_d30', 'td_company_name_over3_d1', 'td_idcard_lend_gurantee_num_d90', 'tzre_rf_m1_ave_apply_request_amount_level', 'td_email_lend_direct_bank_num_d90', 'td_loan_finance_lease_num_d90', 'td_mobile_loan_third_pay_num_d90', 'sd_blacklist_mobile_last6mtenantcount', 'td_mobile_loan_bank_person_small_num_d7', 'td_fifth_contact_mobile_false_family', 'td_lend_device_lease_num_d90', 'td_mobile_lend_traffic_lease_num_d90', 'td_email_lend_bank_person_small_num_d90', 'applthst_use_for', 'td_mobile_loan_person_insurance_num_d7', 'td_idcard_loan_gurantee_num_d90', 'td_idcard_car_ride_break', 'td_loan_car_finance_num_d90', 'td_email_loan_muti_platform_d30', 'td_email_loan_compre_electricity_num_d7', 'sd_loaninfo_pid_d360_monthsfromlastoverdue', 'td_loan_third_service_num_d90', 'sd_loaninfo_mobile_d360_averagetenantgapdays', 'td_mobile_lend_bank_public_num_d90', 'td_device_refuse_over4_d30', 'tzre_rf_m3_ave_apply_request_amount_level', 'td_loan_house_lease_num_d90', 'td_idcard_lend_device_lease_num_d90', 'td_ip_risk_evidence', 'td_idcard_loan_bank_public_num_d7', 'td_mobile_lend_crowd_funding_num_d90', 'td_lend_level1_insurance_d90', 'td_loan_level1_third_service_d30', 'tzre_score_l', 'tzre_rf_m3_max_overdue_repay_amount_level', 'td_mobile_lend_finance_institution_num_d90', 'td_mobile_loan_direct_bank_num_d30', 'ten_fraud_risk_score', 'td_mobile_lend_person_insurance_num_d90', 'td_loan_asset_transfer_num_d90', 'td_email_loan_bank_public_num_d30', 'td_device_conn_mobile_num_d1', 'td_mobile_loan_gurantee_num_d90', 'td_second_contact_court_dishonesty_family', 'td_email_conn_idcard_d90', 'td_mobile_risk_group']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample is 0.3 and colsample_bytree is 0.3 model result is : \n",
      "xgb train auc is ： 0.7669128270263769\n",
      "xgb train ks is ： 0.41245063966339907\n",
      "xgb test auc is ： 0.5561412879403637\n",
      "xgb test ks is ： 0.09937066022619079\n",
      "subsample is 0.4 and colsample_bytree is 0.4 model result is : \n",
      "xgb train auc is ： 0.8239392943848535\n",
      "xgb train ks is ： 0.5022460327417637\n",
      "xgb test auc is ： 0.5476997747381621\n",
      "xgb test ks is ： 0.1257982861522028\n",
      "subsample is 0.5 and colsample_bytree is 0.5 model result is : \n",
      "xgb train auc is ： 0.8749545684422375\n",
      "xgb train ks is ： 0.5920032697787337\n",
      "xgb test auc is ： 0.5515547711386173\n",
      "xgb test ks is ： 0.12269803302292104\n",
      "subsample is 0.6 and colsample_bytree is 0.6 model result is : \n",
      "xgb train auc is ： 0.9138430118180254\n",
      "xgb train ks is ： 0.68020386966931\n",
      "xgb test auc is ： 0.5354206822879172\n",
      "xgb test ks is ： 0.08869394579782164\n",
      "subsample is 0.7 and colsample_bytree is 0.7 model result is : \n",
      "xgb train auc is ： 0.9354836310582416\n",
      "xgb train ks is ： 0.7074362891115592\n",
      "xgb test auc is ： 0.5455864935788765\n",
      "xgb test ks is ： 0.1387102018067392\n",
      "subsample is 0.8 and colsample_bytree is 0.8 model result is : \n",
      "xgb train auc is ： 0.9541564863330233\n",
      "xgb train ks is ： 0.769183111489366\n",
      "xgb test auc is ： 0.5333625554446019\n",
      "xgb test ks is ： 0.08335268572490184\n",
      "subsample is 0.9 and colsample_bytree is 0.9 model result is : \n",
      "xgb train auc is ： 0.9618737073832586\n",
      "xgb train ks is ： 0.8032506360418087\n",
      "xgb test auc is ： 0.5595376326606443\n",
      "xgb test ks is ： 0.1259202062190845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:28:24,254 - __main__[line:1217] - INFO: 将待入模特征重要性持久化\n",
      "2020-12-18 11:28:24,256 - __main__[line:1221] - INFO: 贝叶斯进行模型调参\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample is 1.0 and colsample_bytree is 1.0 model result is : \n",
      "xgb train auc is ： 0.9638691390056924\n",
      "xgb train ks is ： 0.8086274043156423\n",
      "xgb test auc is ： 0.561836696778988\n",
      "xgb test ks is ： 0.10515896054434404\n",
      "Optimizing <__main__.XGBClassifierTuner object at 0x107d040f0>...\n",
      "params_optimizer is :  ['colsample_bytree', 'gamma', 'learning_rate', 'max_depth', 'min_child_weight', 'n_estimators', 'reg_alpha', 'reg_lambda', 'subsample']\n",
      "begain optimizer params!!!\n",
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | n_esti... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "test_auc is :  0.5492324841503913\n",
      "train_auc is :  0.6256976411805867\n",
      "best model result is -145.4457784402648\n",
      "best model result is : \n",
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.5919154032918017, 'gamma': 1.4406489868843162, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.010021731215295529, 'max_delta_step': 0, 'max_depth': 4, 'min_child_weight': 44.8800113543168, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 73, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 0.3725204227553418, 'reg_lambda': 0.6911214540860955, 'scale_pos_weight': 1, 'subsample': 0.6380604845384019, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "current obj_fun result is :  -145.4457784402648\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-145.4   \u001b[0m | \u001b[0m 0.5919  \u001b[0m | \u001b[0m 1.441   \u001b[0m | \u001b[0m 0.01002 \u001b[0m | \u001b[0m 4.419   \u001b[0m | \u001b[0m 44.88   \u001b[0m | \u001b[0m 73.08   \u001b[0m | \u001b[0m 0.3725  \u001b[0m | \u001b[0m 0.6911  \u001b[0m | \u001b[0m 0.6381  \u001b[0m |\n",
      "test_auc is :  0.5\n",
      "train_auc is :  0.5\n",
      "best model result is 49.0\n",
      "best model result is : \n",
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.6771717138023499, 'gamma': 0.8383890288065896, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.1401917050753843, 'max_delta_step': 0, 'max_depth': 4, 'min_child_weight': 263.55711348089267, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 57, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.3409350203568045, 'reg_lambda': 0.8346096047342539, 'scale_pos_weight': 1, 'subsample': 0.735213897067451, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "current obj_fun result is :  49.0\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 49.0    \u001b[0m | \u001b[95m 0.6772  \u001b[0m | \u001b[95m 0.8384  \u001b[0m | \u001b[95m 0.1402  \u001b[0m | \u001b[95m 3.636   \u001b[0m | \u001b[95m 263.6   \u001b[0m | \u001b[95m 56.85   \u001b[0m | \u001b[95m 1.341   \u001b[0m | \u001b[95m 0.8346  \u001b[0m | \u001b[95m 0.7352  \u001b[0m |\n",
      "test_auc is :  0.5544111841341354\n",
      "train_auc is :  0.5607955081932015\n",
      "best model result is 53.884481573564976\n",
      "best model result is : \n",
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.39827085701666365, 'gamma': 0.3962029781697576, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.16214146804835197, 'max_delta_step': 0, 'max_depth': 10, 'min_child_weight': 94.71382926961361, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 223, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.7527783045920766, 'reg_lambda': 1.7892133270076946, 'scale_pos_weight': 1, 'subsample': 0.4510265268218668, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "current obj_fun result is :  53.884481573564976\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 53.88   \u001b[0m | \u001b[95m 0.3983  \u001b[0m | \u001b[95m 0.3962  \u001b[0m | \u001b[95m 0.1621  \u001b[0m | \u001b[95m 9.746   \u001b[0m | \u001b[95m 94.71   \u001b[0m | \u001b[95m 223.1   \u001b[0m | \u001b[95m 1.753   \u001b[0m | \u001b[95m 1.789   \u001b[0m | \u001b[95m 0.451   \u001b[0m |\n",
      "test_auc is :  0.5528726690044355\n",
      "train_auc is :  0.5644820931757597\n",
      "current obj_fun result is :  53.051232448035634\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 53.05   \u001b[0m | \u001b[0m 0.3273  \u001b[0m | \u001b[0m 0.3397  \u001b[0m | \u001b[0m 0.1768  \u001b[0m | \u001b[0m 2.787   \u001b[0m | \u001b[0m 126.9   \u001b[0m | \u001b[0m 289.5   \u001b[0m | \u001b[0m 1.066   \u001b[0m | \u001b[0m 1.384   \u001b[0m | \u001b[0m 0.5893  \u001b[0m |\n",
      "test_auc is :  0.5\n",
      "train_auc is :  0.5\n",
      "current obj_fun result is :  49.0\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 49.0    \u001b[0m | \u001b[0m 0.7806  \u001b[0m | \u001b[0m 1.669   \u001b[0m | \u001b[0m 0.01347 \u001b[0m | \u001b[0m 8.001   \u001b[0m | \u001b[0m 296.7   \u001b[0m | \u001b[0m 237.0   \u001b[0m | \u001b[0m 0.5609  \u001b[0m | \u001b[0m 1.579   \u001b[0m | \u001b[0m 0.4619  \u001b[0m |\n",
      "test_auc is :  0.5\n",
      "train_auc is :  0.5\n",
      "current obj_fun result is :  49.0\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 49.0    \u001b[0m | \u001b[0m 0.8712  \u001b[0m | \u001b[0m 0.6846  \u001b[0m | \u001b[0m 0.1523  \u001b[0m | \u001b[0m 4.079   \u001b[0m | \u001b[0m 264.4   \u001b[0m | \u001b[0m 55.97   \u001b[0m | \u001b[0m 0.9317  \u001b[0m | \u001b[0m 1.384   \u001b[0m | \u001b[0m 0.6833  \u001b[0m |\n",
      "test_auc is :  0.5618192796265762\n",
      "train_auc is :  0.5663430085844626\n",
      "best model result is 54.81363904054197\n",
      "best model result is : \n",
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.3798716222327425, 'gamma': 0.37200062867642086, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.16721432834404562, 'max_delta_step': 0, 'max_depth': 8, 'min_child_weight': 103.06186443243149, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 240, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.607643052581902, 'reg_lambda': 1.7073881624139036, 'scale_pos_weight': 1, 'subsample': 0.4811111514664403, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "current obj_fun result is :  54.81363904054197\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 54.81   \u001b[0m | \u001b[95m 0.3799  \u001b[0m | \u001b[95m 0.372   \u001b[0m | \u001b[95m 0.1672  \u001b[0m | \u001b[95m 8.183   \u001b[0m | \u001b[95m 103.1   \u001b[0m | \u001b[95m 240.3   \u001b[0m | \u001b[95m 1.608   \u001b[0m | \u001b[95m 1.707   \u001b[0m | \u001b[95m 0.4811  \u001b[0m |\n",
      "test_auc is :  0.5586696778987947\n",
      "train_auc is :  0.5674013536987432\n",
      "current obj_fun result is :  54.035287665830886\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 54.04   \u001b[0m | \u001b[0m 0.5718  \u001b[0m | \u001b[0m 0.9596  \u001b[0m | \u001b[0m 0.1285  \u001b[0m | \u001b[0m 8.956   \u001b[0m | \u001b[0m 101.4   \u001b[0m | \u001b[0m 235.5   \u001b[0m | \u001b[0m 0.6756  \u001b[0m | \u001b[0m 0.8031  \u001b[0m | \u001b[0m 0.4935  \u001b[0m |\n",
      "test_auc is :  0.5421785374236548\n",
      "train_auc is :  0.6121855521825903\n",
      "current obj_fun result is :  -73.84439832353922\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-73.84   \u001b[0m | \u001b[0m 0.5587  \u001b[0m | \u001b[0m 0.8242  \u001b[0m | \u001b[0m 0.1039  \u001b[0m | \u001b[0m 4.401   \u001b[0m | \u001b[0m 111.8   \u001b[0m | \u001b[0m 259.0   \u001b[0m | \u001b[0m 1.186   \u001b[0m | \u001b[0m 1.616   \u001b[0m | \u001b[0m 0.9365  \u001b[0m |\n",
      "test_auc is :  0.5417285943196861\n",
      "train_auc is :  0.5934066360469823\n",
      "current obj_fun result is :  18.225742224300348\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 18.23   \u001b[0m | \u001b[0m 0.5464  \u001b[0m | \u001b[0m 0.3982  \u001b[0m | \u001b[0m 0.09063 \u001b[0m | \u001b[0m 8.113   \u001b[0m | \u001b[0m 97.58   \u001b[0m | \u001b[0m 237.1   \u001b[0m | \u001b[0m 1.124   \u001b[0m | \u001b[0m 0.7193  \u001b[0m | \u001b[0m 0.626   \u001b[0m |\n",
      "test_auc is :  0.5433803209400618\n",
      "train_auc is :  0.6449790529799869\n",
      "current obj_fun result is :  -1089.663448533767\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-1.09e+03\u001b[0m | \u001b[0m 0.8229  \u001b[0m | \u001b[0m 0.3734  \u001b[0m | \u001b[0m 0.1048  \u001b[0m | \u001b[0m 8.146   \u001b[0m | \u001b[0m 102.5   \u001b[0m | \u001b[0m 235.9   \u001b[0m | \u001b[0m 0.2262  \u001b[0m | \u001b[0m 0.8058  \u001b[0m | \u001b[0m 0.8824  \u001b[0m |\n",
      "test_auc is :  0.5\n",
      "train_auc is :  0.5\n",
      "current obj_fun result is :  49.0\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 49.0    \u001b[0m | \u001b[0m 0.966   \u001b[0m | \u001b[0m 1.114   \u001b[0m | \u001b[0m 0.1702  \u001b[0m | \u001b[0m 3.382   \u001b[0m | \u001b[0m 243.0   \u001b[0m | \u001b[0m 135.6   \u001b[0m | \u001b[0m 1.498   \u001b[0m | \u001b[0m 1.67    \u001b[0m | \u001b[0m 0.5894  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:29:32,681 - __main__[line:1225] - INFO: 模型调参完成！！！\n",
      "2020-12-18 11:29:32,683 - __main__[line:1226] - INFO: 模型参数：{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.3798716222327425, 'gamma': 0.37200062867642086, 'gpu_id': -1, 'interaction_constraints': '', 'learning_rate': 0.16721432834404562, 'max_delta_step': 0, 'max_depth': 8, 'min_child_weight': 103.06186443243149, 'monotone_constraints': '()', 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.607643052581902, 'reg_lambda': 1.7073881624139036, 'scale_pos_weight': 1, 'subsample': 0.4811111514664403, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "2020-12-18 11:29:32,687 - __main__[line:1227] - INFO: 模型参数：{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.3798716222327425, 'gamma': 0.37200062867642086, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.16721432834404562, 'max_delta_step': 0, 'max_depth': 8, 'min_child_weight': 103.06186443243149, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 240, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.607643052581902, 'reg_lambda': 1.7073881624139036, 'scale_pos_weight': 1, 'subsample': 0.4811111514664403, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "2020-12-18 11:29:32,693 - __main__[line:1235] - INFO: 模型训练完成, 使用 228 秒\n",
      "2020-12-18 11:29:32,865 - __main__[line:1121] - INFO: 模型相关结果统计！！！\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc is :  0.5735061888948236\n",
      "train_auc is :  0.7702120311818931\n",
      "current obj_fun result is :  -834460.9885638981\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-8.345e+0\u001b[0m | \u001b[0m 0.7273  \u001b[0m | \u001b[0m 0.09023 \u001b[0m | \u001b[0m 0.09288 \u001b[0m | \u001b[0m 5.767   \u001b[0m | \u001b[0m 38.62   \u001b[0m | \u001b[0m 128.8   \u001b[0m | \u001b[0m 0.4834  \u001b[0m | \u001b[0m 1.68    \u001b[0m | \u001b[0m 0.9111  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "optimizer params over!!! 共耗时1.140183401107788 分钟\n",
      "the best params is : {'colsample_bytree': 0.3798716222327425, 'gamma': 0.37200062867642086, 'learning_rate': 0.16721432834404562, 'max_depth': 8.183113611411272, 'min_child_weight': 103.06186443243149, 'n_estimators': 240.34798201956818, 'reg_alpha': 1.607643052581902, 'reg_lambda': 1.7073881624139036, 'subsample': 0.4811111514664403}\n",
      "Maximum xgb value is : 54.81363904054197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:29:33,017 - __main__[line:1127] - INFO: 模型效果：\n",
      "2020-12-18 11:29:33,072 - __main__[line:1135] - INFO: 不同月份的模型效果：\n",
      "2020-12-18 11:29:33,083 - __main__[line:1140] - INFO: 分数describe\n",
      "2020-12-18 11:29:33,187 - __main__[line:1148] - INFO: 模型psi：\n",
      "2020-12-18 11:29:33,200 - __main__[line:1164] - INFO: 统计结束\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            auc        ks\n",
      "type                     \n",
      "train  0.566522  0.097896\n",
      "test   0.558269  0.127528\n",
      "oot         NaN       NaN\n",
      "cv          NaN       NaN\n",
      "              auc        ks\n",
      "month                      \n",
      "2019/1/  0.539880  0.116066\n",
      "2019/2/  0.578972  0.169497\n",
      "2019/3/  0.552336  0.097755\n",
      "2019/4/  0.543540  0.086556\n",
      "2019/5/  0.587759  0.182127\n",
      "               lhpdat        score\n",
      "count     5218.000000  5218.000000\n",
      "mean         0.112852   646.496359\n",
      "std          0.007775     5.603513\n",
      "min          0.093243   631.000000\n",
      "25%          0.107106   642.000000\n",
      "50%          0.112727   647.000000\n",
      "75%          0.118512   651.000000\n",
      "max          0.133209   660.000000\n",
      "coverage     1.000000     1.000000\n",
      "   train_test_psi\n",
      "0         0.00411\n",
      "过滤前30个特征出来，再次训练\n",
      "过滤后的特征： ['ten_fraud_risk_score', 'tzre_rf_m3_ave_repay_amount_level', 'sd_loaninfo_mobile_d360_averagetenantgapdays', 'td_final_score', 'tzre_rf_m6_max_repay_amount_level', 'tzre_rf_m24_max_repay_amount_level', 'tzre_score_l']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:29:33,229 - __main__[line:1278] - INFO: 贝叶斯进行模型调参\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体数据集大小： (5218, 7)\n",
      "训练集大小： (3913, 7)\n",
      "测试集大小： (1305, 7)\n",
      "无oot数据集\n",
      "Optimizing <__main__.XGBClassifierTuner object at 0x107d1b438>...\n",
      "params_optimizer is :  ['colsample_bytree', 'gamma', 'learning_rate', 'max_depth', 'min_child_weight', 'n_estimators', 'reg_alpha', 'reg_lambda', 'subsample']\n",
      "begain optimizer params!!!\n",
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | n_esti... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "test_auc is :  0.5447562759805856\n",
      "train_auc is :  0.5852674350474234\n",
      "best model result is 37.89857162743636\n",
      "best model result is : \n",
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.5919154032918017, 'gamma': 1.4406489868843162, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.010021731215295529, 'max_delta_step': 0, 'max_depth': 4, 'min_child_weight': 44.8800113543168, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 73, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 0.3725204227553418, 'reg_lambda': 0.6911214540860955, 'scale_pos_weight': 1, 'subsample': 0.6380604845384019, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "current obj_fun result is :  37.89857162743636\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 37.9    \u001b[0m | \u001b[0m 0.5919  \u001b[0m | \u001b[0m 1.441   \u001b[0m | \u001b[0m 0.01002 \u001b[0m | \u001b[0m 4.419   \u001b[0m | \u001b[0m 44.88   \u001b[0m | \u001b[0m 73.08   \u001b[0m | \u001b[0m 0.3725  \u001b[0m | \u001b[0m 0.6911  \u001b[0m | \u001b[0m 0.6381  \u001b[0m |\n",
      "test_auc is :  0.5\n",
      "train_auc is :  0.5\n",
      "best model result is 49.0\n",
      "best model result is : \n",
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.6771717138023499, 'gamma': 0.8383890288065896, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.1401917050753843, 'max_delta_step': 0, 'max_depth': 4, 'min_child_weight': 263.55711348089267, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 57, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.3409350203568045, 'reg_lambda': 0.8346096047342539, 'scale_pos_weight': 1, 'subsample': 0.735213897067451, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "current obj_fun result is :  49.0\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 49.0    \u001b[0m | \u001b[95m 0.6772  \u001b[0m | \u001b[95m 0.8384  \u001b[0m | \u001b[95m 0.1402  \u001b[0m | \u001b[95m 3.636   \u001b[0m | \u001b[95m 263.6   \u001b[0m | \u001b[95m 56.85   \u001b[0m | \u001b[95m 1.341   \u001b[0m | \u001b[95m 0.8346  \u001b[0m | \u001b[95m 0.7352  \u001b[0m |\n",
      "test_auc is :  0.5489131696895103\n",
      "train_auc is :  0.5546533555974913\n",
      "best model result is 53.402652530741015\n",
      "best model result is : \n",
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.39827085701666365, 'gamma': 0.3962029781697576, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.16214146804835197, 'max_delta_step': 0, 'max_depth': 10, 'min_child_weight': 94.71382926961361, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 223, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.7527783045920766, 'reg_lambda': 1.7892133270076946, 'scale_pos_weight': 1, 'subsample': 0.4510265268218668, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "current obj_fun result is :  53.402652530741015\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 53.4    \u001b[0m | \u001b[95m 0.3983  \u001b[0m | \u001b[95m 0.3962  \u001b[0m | \u001b[95m 0.1621  \u001b[0m | \u001b[95m 9.746   \u001b[0m | \u001b[95m 94.71   \u001b[0m | \u001b[95m 223.1   \u001b[0m | \u001b[95m 1.753   \u001b[0m | \u001b[95m 1.789   \u001b[0m | \u001b[95m 0.451   \u001b[0m |\n",
      "test_auc is :  0.5722028053226818\n",
      "train_auc is :  0.5562633465305811\n",
      "best model result is 54.201541841327156\n",
      "best model result is : \n",
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.32733834826301766, 'gamma': 0.3396608391291378, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.1768470756515885, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 126.9111798765106, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 289, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.0663305699460341, 'reg_lambda': 1.3837542279009467, 'scale_pos_weight': 1, 'subsample': 0.5893093786036377, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "current obj_fun result is :  54.201541841327156\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 54.2    \u001b[0m | \u001b[95m 0.3273  \u001b[0m | \u001b[95m 0.3397  \u001b[0m | \u001b[95m 0.1768  \u001b[0m | \u001b[95m 2.787   \u001b[0m | \u001b[95m 126.9   \u001b[0m | \u001b[95m 289.5   \u001b[0m | \u001b[95m 1.066   \u001b[0m | \u001b[95m 1.384   \u001b[0m | \u001b[95m 0.5893  \u001b[0m |\n",
      "test_auc is :  0.5\n",
      "train_auc is :  0.5\n",
      "current obj_fun result is :  49.0\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 49.0    \u001b[0m | \u001b[0m 0.7806  \u001b[0m | \u001b[0m 1.669   \u001b[0m | \u001b[0m 0.01347 \u001b[0m | \u001b[0m 8.001   \u001b[0m | \u001b[0m 296.7   \u001b[0m | \u001b[0m 237.0   \u001b[0m | \u001b[0m 0.5609  \u001b[0m | \u001b[0m 1.579   \u001b[0m | \u001b[0m 0.4619  \u001b[0m |\n",
      "test_auc is :  0.5638832121873621\n",
      "train_auc is :  0.565381670355084\n",
      "best model result is 55.278870321874166\n",
      "best model result is : \n",
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.8103020377723005, 'gamma': 1.159552505628805, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.021984879343277444, 'max_delta_step': 0, 'max_depth': 9, 'min_child_weight': 94.9059528743507, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 226, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.6107401281088571, 'reg_lambda': 1.2652793852317905, 'scale_pos_weight': 1, 'subsample': 0.4112793920607084, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "current obj_fun result is :  55.278870321874166\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 55.28   \u001b[0m | \u001b[95m 0.8103  \u001b[0m | \u001b[95m 1.16    \u001b[0m | \u001b[95m 0.02198 \u001b[0m | \u001b[95m 8.715   \u001b[0m | \u001b[95m 94.91   \u001b[0m | \u001b[95m 225.7   \u001b[0m | \u001b[95m 1.611   \u001b[0m | \u001b[95m 1.265   \u001b[0m | \u001b[95m 0.4113  \u001b[0m |\n",
      "test_auc is :  0.5312347599916397\n",
      "train_auc is :  0.6174116364285427\n",
      "current obj_fun result is :  -339.68646852250794\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-339.7   \u001b[0m | \u001b[0m 0.8125  \u001b[0m | \u001b[0m 0.6606  \u001b[0m | \u001b[0m 0.09987 \u001b[0m | \u001b[0m 8.339   \u001b[0m | \u001b[0m 93.74   \u001b[0m | \u001b[0m 225.0   \u001b[0m | \u001b[0m 1.968   \u001b[0m | \u001b[0m 0.6095  \u001b[0m | \u001b[0m 0.8933  \u001b[0m |\n",
      "test_auc is :  0.5467476370729896\n",
      "train_auc is :  0.7906174294210241\n",
      "current obj_fun result is :  -21938714.222272534\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.194e+0\u001b[0m | \u001b[0m 0.8776  \u001b[0m | \u001b[0m 1.598   \u001b[0m | \u001b[0m 0.05579 \u001b[0m | \u001b[0m 7.971   \u001b[0m | \u001b[0m 20.12   \u001b[0m | \u001b[0m 170.9   \u001b[0m | \u001b[0m 0.1648  \u001b[0m | \u001b[0m 1.248   \u001b[0m | \u001b[0m 0.8105  \u001b[0m |\n",
      "test_auc is :  0.5478594319686027\n",
      "train_auc is :  0.5943640940009649\n",
      "current obj_fun result is :  29.671137589711577\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 29.67   \u001b[0m | \u001b[0m 0.6518  \u001b[0m | \u001b[0m 1.491   \u001b[0m | \u001b[0m 0.07469 \u001b[0m | \u001b[0m 7.489   \u001b[0m | \u001b[0m 87.7    \u001b[0m | \u001b[0m 182.8   \u001b[0m | \u001b[0m 1.892   \u001b[0m | \u001b[0m 0.7896  \u001b[0m | \u001b[0m 0.948   \u001b[0m |\n",
      "test_auc is :  0.5575346601332993\n",
      "train_auc is :  0.5704172977503458\n",
      "current obj_fun result is :  53.311126506421246\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 53.31   \u001b[0m | \u001b[0m 0.6698  \u001b[0m | \u001b[0m 0.7309  \u001b[0m | \u001b[0m 0.11    \u001b[0m | \u001b[0m 5.26    \u001b[0m | \u001b[0m 179.0   \u001b[0m | \u001b[0m 78.71   \u001b[0m | \u001b[0m 0.3457  \u001b[0m | \u001b[0m 1.893   \u001b[0m | \u001b[0m 0.85    \u001b[0m |\n",
      "test_auc is :  0.5590064095120875\n",
      "train_auc is :  0.5671158301008484\n",
      "current obj_fun result is :  54.14628431314975\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 54.15   \u001b[0m | \u001b[0m 0.9488  \u001b[0m | \u001b[0m 1.923   \u001b[0m | \u001b[0m 0.08349 \u001b[0m | \u001b[0m 8.07    \u001b[0m | \u001b[0m 96.22   \u001b[0m | \u001b[0m 254.4   \u001b[0m | \u001b[0m 1.356   \u001b[0m | \u001b[0m 0.6257  \u001b[0m | \u001b[0m 0.5153  \u001b[0m |\n",
      "test_auc is :  0.5426197719514178\n",
      "train_auc is :  0.5911421719970872\n",
      "current obj_fun result is :  25.377149079660384\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 25.38   \u001b[0m | \u001b[0m 0.8517  \u001b[0m | \u001b[0m 1.013   \u001b[0m | \u001b[0m 0.01248 \u001b[0m | \u001b[0m 8.477   \u001b[0m | \u001b[0m 96.16   \u001b[0m | \u001b[0m 251.6   \u001b[0m | \u001b[0m 0.4399  \u001b[0m | \u001b[0m 1.848   \u001b[0m | \u001b[0m 0.9991  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:29:36,555 - __main__[line:1282] - INFO: 模型调参完成！！！\n",
      "2020-12-18 11:29:36,557 - __main__[line:1283] - INFO: 模型参数：{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.8103020377723005, 'gamma': 1.159552505628805, 'gpu_id': -1, 'interaction_constraints': '', 'learning_rate': 0.021984879343277444, 'max_delta_step': 0, 'max_depth': 9, 'min_child_weight': 94.9059528743507, 'monotone_constraints': '()', 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.6107401281088571, 'reg_lambda': 1.2652793852317905, 'scale_pos_weight': 1, 'subsample': 0.4112793920607084, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "2020-12-18 11:29:36,559 - __main__[line:1284] - INFO: 模型参数：{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.8103020377723005, 'gamma': 1.159552505628805, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.021984879343277444, 'max_delta_step': 0, 'max_depth': 9, 'min_child_weight': 94.9059528743507, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 226, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 1.6107401281088571, 'reg_lambda': 1.2652793852317905, 'scale_pos_weight': 1, 'subsample': 0.4112793920607084, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'nthread': -1}\n",
      "2020-12-18 11:29:36,559 - __main__[line:1287] - INFO: 模型训练完成, 使用 3 秒\n",
      "2020-12-18 11:29:36,604 - __main__[line:1121] - INFO: 模型相关结果统计！！！\n",
      "2020-12-18 11:29:36,676 - __main__[line:1127] - INFO: 模型效果：\n",
      "2020-12-18 11:29:36,727 - __main__[line:1135] - INFO: 不同月份的模型效果：\n",
      "2020-12-18 11:29:36,737 - __main__[line:1140] - INFO: 分数describe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc is :  0.5489567125705395\n",
      "train_auc is :  0.5798677084774964\n",
      "current obj_fun result is :  46.37421740564427\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 46.37   \u001b[0m | \u001b[0m 0.7417  \u001b[0m | \u001b[0m 1.594   \u001b[0m | \u001b[0m 0.07886 \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 91.25   \u001b[0m | \u001b[0m 184.0   \u001b[0m | \u001b[0m 1.155   \u001b[0m | \u001b[0m 0.4163  \u001b[0m | \u001b[0m 0.5393  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "optimizer params over!!! 共耗时0.05535011688868205 分钟\n",
      "the best params is : {'colsample_bytree': 0.8103020377723005, 'gamma': 1.159552505628805, 'learning_rate': 0.021984879343277444, 'max_depth': 8.715331626504032, 'min_child_weight': 94.9059528743507, 'n_estimators': 225.74970160518808, 'reg_alpha': 1.6107401281088571, 'reg_lambda': 1.2652793852317905, 'subsample': 0.4112793920607084}\n",
      "Maximum xgb value is : 55.278870321874166\n",
      "            auc        ks\n",
      "type                     \n",
      "train  0.564146  0.102865\n",
      "test   0.564536  0.117508\n",
      "oot         NaN       NaN\n",
      "cv          NaN       NaN\n",
      "              auc        ks\n",
      "month                      \n",
      "2019/1/  0.531079  0.112513\n",
      "2019/2/  0.581998  0.172399\n",
      "2019/3/  0.561400  0.115393\n",
      "2019/4/  0.531981  0.086890\n",
      "2019/5/  0.574589  0.133619\n",
      "               lhpdat        score\n",
      "count     5218.000000  5218.000000\n",
      "mean         0.117108   649.588731\n",
      "std          0.004133     2.900610\n",
      "min          0.107155   642.000000\n",
      "25%          0.113849   647.000000\n",
      "50%          0.117269   650.000000\n",
      "75%          0.120267   652.000000\n",
      "max          0.127409   657.000000\n",
      "coverage     1.000000     1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 11:29:36,817 - __main__[line:1148] - INFO: 模型psi：\n",
      "2020-12-18 11:29:36,834 - __main__[line:1164] - INFO: 统计结束\n",
      "2020-12-18 11:29:36,834 - __main__[line:1304] - INFO: 模型相关结果持久化\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train_test_psi\n",
      "0        0.006543\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_evaluator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0098fa478c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_evaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_save\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m     \u001b[0msave_pythonmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'td06p1_xgb_model_v1.ml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscript_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscript_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model_evaluator'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "# !/usr/bin/env python\n",
    "# ! -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    "@File: model_fit_v3.py\n",
    "@Author: RyanZheng\n",
    "@Email: ryan.zhengrp@gmail.com\n",
    "@Created Time on: 2020-07-26\n",
    "\n",
    "y值加密混淆\n",
    "ok 可以跑通\n",
    "'''\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from joblib import Parallel, delayed\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 日志输出\n",
    "class Logger():\n",
    "    # 日志级别关系映射\n",
    "    level_relations = {\n",
    "        \"debug\": logging.DEBUG,\n",
    "        \"info\": logging.INFO,\n",
    "        \"warning\": logging.WARNING,\n",
    "        \"error\": logging.ERROR,\n",
    "        \"critical\": logging.CRITICAL\n",
    "    }\n",
    "\n",
    "    def __init__(self, level=\"info\", name=None,\n",
    "                 fmt=\"%(asctime)s - %(name)s[line:%(lineno)d] - %\"\n",
    "                     \"(levelname)s: %(message)s\"):\n",
    "        logging.basicConfig(level=self.level_relations.get(level), format=fmt)\n",
    "        self.logger = logging.getLogger(name)\n",
    "\n",
    "\n",
    "log = Logger(level='info', name=__name__).logger\n",
    "\n",
    "\n",
    "def get_ks(target, y_pred):\n",
    "    df = pd.DataFrame({\n",
    "        'y_pred': y_pred,\n",
    "        'target': target,\n",
    "    })\n",
    "    df = df.sort_values(by='y_pred', ascending=False)\n",
    "    df['good'] = 1 - df['target']\n",
    "    df['bad_rate'] = df['target'].cumsum() / df['target'].sum()\n",
    "    df['good_rate'] = df['good'].cumsum() / df['good'].sum()\n",
    "    df['ks'] = df['bad_rate'] - df['good_rate']\n",
    "    return max(abs(df['ks']))\n",
    "\n",
    "\n",
    "# auc\n",
    "def get_roc_auc_score(target, y_pred):\n",
    "    if target.nunique() != 2:\n",
    "        raise ValueError('the target is not 2 classier target')\n",
    "    else:\n",
    "        return roc_auc_score(target, y_pred)\n",
    "\n",
    "\n",
    "def get_splitted_data(df_selected, target, selected_features):\n",
    "    X = {}\n",
    "    y = {}\n",
    "\n",
    "    X['all'] = df_selected[selected_features]\n",
    "    y['all'] = df_selected[target]\n",
    "\n",
    "    for name, df in df_selected.groupby('type'):\n",
    "        X[name] = df[selected_features]\n",
    "        y[name] = df[target]\n",
    "\n",
    "    if not X.__contains__('oot'):\n",
    "        X['oot'] = None\n",
    "        y['oot'] = None\n",
    "\n",
    "    return X['all'], y['all'], X['train'], y['train'], X['test'], y['test'], X['oot'], y['oot']\n",
    "\n",
    "\n",
    "def to_score(x):\n",
    "    import math\n",
    "    if x <= 0.001:\n",
    "        x = 0.001\n",
    "    elif x >= 0.999:\n",
    "        x = 0.999\n",
    "\n",
    "    A = 404.65547022\n",
    "    B = 72.1347520444\n",
    "    result = int(round(A - B * math.log(x / (1 - x))))\n",
    "\n",
    "    if result < 0:\n",
    "        result = 0\n",
    "    if result > 1200:\n",
    "        result = 1200\n",
    "    result = 1200 - result\n",
    "    return result\n",
    "\n",
    "\n",
    "def psi_statis(df_src, splitted_types, scores):\n",
    "    def bin_psi(x, y):\n",
    "\n",
    "        if pd.isnull(y) or y == 0 or pd.isnull(x) or x == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return (x - y) * math.log(x / y)\n",
    "\n",
    "    if 'train' not in splitted_types:\n",
    "        print('Error: failt to get psi, for train is not in splitted_types')\n",
    "        return\n",
    "\n",
    "    bins = list(range(300, 951, 50))\n",
    "    l = []\n",
    "    for (client_batch, splitted_type), df_type in df_src.groupby(['client_batch', 'type']):\n",
    "        for score in scores:\n",
    "            df_score = df_type[df_type[score].notnull()]\n",
    "            df = pd.cut(df_score[score].map(to_score), bins=bins, right=False).value_counts().map(\n",
    "                lambda v: v / df_score.shape[0] if df_score.shape[0] > 0 else np.nan).to_frame('pct')\n",
    "            df.index.name = 'bin'\n",
    "            df.index = df.index.astype(str)\n",
    "            df = df.reset_index()\n",
    "\n",
    "            df['client_batch'] = client_batch\n",
    "            df['type'] = splitted_type\n",
    "            df['feature'] = score\n",
    "\n",
    "            l.append(df)\n",
    "\n",
    "    df_psi_detail = pd.concat(l, ignore_index=True).pivot_table(index=['client_batch', 'feature', 'bin'],\n",
    "                                                                columns='type', values='pct')\n",
    "    df_psi_detail.columns = [s + '_pct' for s in df_psi_detail.columns.format()]\n",
    "    df_psi_detail = df_psi_detail.reset_index()\n",
    "\n",
    "    for splitted_type in filter(lambda s: s != 'train', splitted_types):\n",
    "        df_psi_detail['train_{}_psi'.format(splitted_type)] = df_psi_detail.apply(\n",
    "            lambda r: bin_psi(r['train_pct'], r[splitted_type + '_pct']), axis=1)\n",
    "\n",
    "    psi_col = list(filter(lambda col: '_psi' in col, df_psi_detail.columns.format()))\n",
    "    df_psi = df_psi_detail.groupby(['client_batch', 'feature']).sum()[psi_col].reset_index()\n",
    "\n",
    "    df_psi_detail_sum = df_psi_detail.drop(labels='bin', axis=1).groupby(\n",
    "        ['client_batch', 'feature']).sum().reset_index()\n",
    "    df_psi_detail_sum['bin'] = '[sum]'\n",
    "\n",
    "    df_psi_detail = pd.concat([df_psi_detail, df_psi_detail_sum], ignore_index=True).sort_values(\n",
    "        ['client_batch', 'feature'])\n",
    "    df_psi_detail = pd.DataFrame(df_psi_detail, columns=['client_batch', 'feature', 'bin',\n",
    "                                                         'train_pct', 'test_pct', 'oot_pct', 'train_test_psi',\n",
    "                                                         'train_oot_psi'])\n",
    "\n",
    "    return df_psi, df_psi_detail\n",
    "\n",
    "\n",
    "def train_test_split_(df_src, target='y_target', test_size=0.3):\n",
    "    \"\"\"\n",
    "    样本切分函数.先按target分类，每类单独切成train/test，再按train/test合并，\n",
    "    使得train/test的badrate能高度一致\n",
    "    :param df_src:\n",
    "    :param target:\n",
    "    :param test_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    l = [[], [], [], []]\n",
    "    for target_value, X in df_src.groupby(target):\n",
    "\n",
    "        X[target] = target_value\n",
    "\n",
    "        row = train_test_split(X.drop(labels=target, axis=1), X[target], test_size=test_size, random_state=1234)\n",
    "\n",
    "        for i in range(0, 4):\n",
    "            l[i].append(row[i])\n",
    "\n",
    "    list_df = []\n",
    "    for i in range(0, 4):\n",
    "        list_df.append(pd.concat(l[i]))\n",
    "\n",
    "    return tuple(list_df)\n",
    "\n",
    "\n",
    "def split_data_type(df, key_col='tdid', target='target', apply_time='apply_time', test_size=0.3):\n",
    "    df_id = df.copy()\n",
    "    if df_id[target].isin([0, 1]).all():\n",
    "        print('样本y值在0，1')\n",
    "    else:\n",
    "        print('\\033[0;31m样本y值不在0，1之间，请检查！！！\\033[0m')\n",
    "\n",
    "    print('样本情况：', df_id.shape)\n",
    "    df_id.drop_duplicates(subset=key_col, inplace=True)\n",
    "    print('分布情况：', df_id.groupby(target)[key_col].count().sort_index())\n",
    "    # df_id.groupby(target)['tdid'].count().sort_index().to_excel(\n",
    "    #     '{}{}_id_distributed.xlsx'.format(data_dir, client_batch))\n",
    "    print('样本drop_duplicates情况：', df_id.shape)\n",
    "\n",
    "    df_id = df_id.loc[df_id[target].isin([0, 1])]\n",
    "    print('样本y值在0，1的样本情况：', df_id.shape)\n",
    "\n",
    "    # ---------查看各月badrate---------------------\n",
    "    df_id['apply_month'] = df_id[apply_time].map(lambda s: s[:7])\n",
    "    print(df_id.groupby('apply_month').describe()[target])\n",
    "\n",
    "    # ---------样本划分----------------------------\n",
    "    ##需要oot\n",
    "    # df_selected = df_id #can filter records here\n",
    "    # # df_oot = df_selected[df_selected['apply_time']>= '2019-04-01']\n",
    "    # # X_train = df_selected[df_selected['apply_time']<= '2019-01-31']\n",
    "    # # X_test = df_selected[(df_selected['apply_time']> '2019-01-31') & (df_selected['apply_time']< '2019-04-01')]\n",
    "\n",
    "    # df_oot = df_selected[df_selected['apply_time']>= '2019-03-01']\n",
    "    # X_train = df_selected[df_selected['apply_time']<= '2018-12-31']\n",
    "    # X_test = df_selected[(df_selected['apply_time']> '2018-12-31') & (df_selected['apply_time']< '2019-03-01')]\n",
    "\n",
    "    # #X_train, X_test, y_train, y_test = geo_train_test_split(df_not_oot,label=label)\n",
    "\n",
    "    # df_id.loc[df_oot.index,'type'] = 'oot'\n",
    "    ##需要oot\n",
    "\n",
    "    # 不需要oot的时候运行下面这一行代码\n",
    "    X_train, X_test, y_train, y_test = train_test_split_(df_id, target=target, test_size=test_size)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(df_id.drop(columns=target), df_id[target], test_size=test_size,\n",
    "    #                                                     random_state=123)\n",
    "    # 不需要oot的时候运行下面这一行代码\n",
    "\n",
    "    df_id.loc[X_train.index, 'type'] = 'train'\n",
    "    df_id.loc[X_test.index, 'type'] = 'test'\n",
    "\n",
    "    print(df_id.groupby('type').describe()[target])\n",
    "\n",
    "    # ----------输出---------------------------------\n",
    "    # df_id.to_csv(data_dir + '{}_split.csv'.format(client_batch), index=False)\n",
    "    return df_id\n",
    "\n",
    "\n",
    "def select_features_dtypes(df, exclude=None):\n",
    "    '''\n",
    "    根据数据集，筛选出数据类型\n",
    "    :param df: 数据集\n",
    "    :param exclude: 排除不需要参与筛选的列\n",
    "    :return:三个list\n",
    "    '''\n",
    "    if exclude is not None:\n",
    "        df = df.drop(columns=exclude)\n",
    "    # 筛选出数值类型列\n",
    "    numeric_df = df.select_dtypes([np.number])\n",
    "\n",
    "    no_numeric_df = df.select_dtypes(include=['object'])\n",
    "    # 将object类型的列尝试转成时间类型\n",
    "    dates_objs_df = no_numeric_df.apply(pd.to_datetime, errors='ignore')\n",
    "    # 筛选出字符类型列\n",
    "    objs_df = dates_objs_df.select_dtypes(include=['object'])\n",
    "    # 筛选出时间类型列\n",
    "    dates_df = list(set(dates_objs_df.columns) - set(objs_df.columns))\n",
    "\n",
    "    assert len(numeric_df.columns) + len(objs_df.columns) + len(dates_df) == df.shape[1]\n",
    "\n",
    "    return numeric_df.columns.tolist(), objs_df.columns.tolist(), dates_df\n",
    "\n",
    "\n",
    "def category_2_woe(df, category_cols=[], target='target'):\n",
    "    '''\n",
    "    方法说明。每个类别都会转成woe值。缺失值不转，即还是为缺失值。在考虑到未来如果有新类别，给予other对应woe为0\n",
    "    :param df:\n",
    "    :param category_cols:\n",
    "    :param target:\n",
    "    :return:\n",
    "    '''\n",
    "    var_value_woe = {}\n",
    "    for i in category_cols:\n",
    "        # bin_g = df.groupby(by=i)[target].agg({'total_cnt': 'count', 'bad_cnt': 'sum'})\n",
    "        # https://stackoverflow.com/questions/60229375/solution-for-specificationerror-nested-renamer-is-not-supported-while-agg-alo\n",
    "        bin_g = df.groupby(by=i)[target].agg([('total_cnt', 'count'), ('bad_cnt', 'sum')])\n",
    "        bin_g['good_cnt'] = bin_g['total_cnt'] - bin_g['bad_cnt']\n",
    "        bin_g['bad_rate'] = bin_g['bad_cnt'] / sum(bin_g['bad_cnt'])\n",
    "        bin_g['good_rate'] = bin_g['good_cnt'] / sum(bin_g['good_cnt'])\n",
    "        bin_g['good_rate'].replace({0: 0.0000000001}, inplace=True)  # good_rate为0的情况下，woe算出来是-inf。即将0使用一个极小数替换\n",
    "        bin_g['woe'] = bin_g.apply(lambda x: 0.0 if x['bad_rate'] == 0 else np.log(x['good_rate'] / x['bad_rate']),\n",
    "                                   axis=1)\n",
    "\n",
    "        value_woe = bin_g['woe'].to_dict()\n",
    "        value_woe['other'] = 0  # 未来有新类别的情况下，woe值给予0\n",
    "        var_value_woe[i] = value_woe\n",
    "\n",
    "    return var_value_woe\n",
    "\n",
    "\n",
    "def category_2_woe_save(var_value_woe, path=None):\n",
    "    if path is None:\n",
    "        path = sys.path[0]\n",
    "\n",
    "    with open(path + 'category_var_value_woe.json', 'w') as f:\n",
    "        json.dump(var_value_woe, f)\n",
    "\n",
    "\n",
    "def category_2_woe_load(path=None):\n",
    "    with open(path + 'category_var_value_woe.json', 'r') as f:\n",
    "        var_value_woe = json.load(f)\n",
    "    return var_value_woe\n",
    "\n",
    "\n",
    "def filter_miss(df, miss_threshold=0.9):\n",
    "    '''\n",
    "\n",
    "    :param df: 数据集\n",
    "    :param miss_threshold: 缺失率大于等于该阈值的变量剔除\n",
    "    :return:\n",
    "    '''\n",
    "    names_list = []\n",
    "    for name, series in df.items():\n",
    "        n = series.isnull().sum()\n",
    "        miss_q = n / series.size\n",
    "        if miss_q < miss_threshold:\n",
    "            names_list.append(name)\n",
    "    return names_list\n",
    "\n",
    "\n",
    "# =============================\n",
    "\n",
    "class WoeTransformer(TransformerMixin):\n",
    "\n",
    "    def __init__(self, n_jobs=2):\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def bin_to_woe(self, df, var_bin_woe_dict):\n",
    "        '''\n",
    "        根据传进来的var_bin_woe_dict对原始值进行映射。\n",
    "        如在var_bin_woe_dict没有的类别（数据集中新出现的类别，归为到other这类）同时var_bin_woe_dict中得有other该类别对应的woe值\n",
    "        如果var_bin_woe_dict中没有other该类别对应的woe值，即数据集中新出现的类别归为缺失值，即新出现的类别没有woe值\n",
    "        :param df:\n",
    "        :param var_bin_woe_dict:    形如{\"Sex\": {\"female\": -1.5298770033401874, \"male\": 0.9838327092415774}, \"Embarked\": {\"C\": -0.694264203516269, \"S\": 0.1977338357888416, \"other\": -0.030202603851420356}}\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        for feature, bin_woe in var_bin_woe_dict.items():\n",
    "            df[feature] = df[feature].map(\n",
    "                lambda x: x if (x in bin_woe.keys() or x is np.nan or pd.isna(x)) else 'other')\n",
    "            df[feature] = df[feature].map(bin_woe)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, df, var_bin_woe_dict, bins_dict={}):\n",
    "        '''\n",
    "        输入三列的dataframe，['var_name','range','woe'] 返回转换woe后的数据集\n",
    "        :param var_bin_woe_dict:    形如{\"Sex\": {\"female\": -1.5298770033401874, \"male\": 0.9838327092415774}, \"Embarked\": {\"C\": -0.694264203516269, \"S\": 0.1977338357888416, \"other\": -0.030202603851420356}}\n",
    "        :return:转换woe后的数据集\n",
    "        '''\n",
    "\n",
    "        df_ = df.copy()\n",
    "        if bins_dict:\n",
    "            print('需要将原始数据转bin')\n",
    "            df_ = self.data_to_bin(df, bins_dict=bins_dict)\n",
    "        return self.bin_to_woe(df_, var_bin_woe_dict)\n",
    "\n",
    "    def data_to_bin(self, df, bins_dict={}):\n",
    "        '''\n",
    "        原始数据根据bins_dict进行分箱\n",
    "        :param df:含有目标变量的数据集；不需要返回var_summary可以不需要目标变量，将函数中target部分注释\n",
    "        :param target:目标值变量名称\n",
    "        :param bins_dict:分箱字典, 形如{'D157': [-999, 1.0, 2.0, 3.0, 5.0, inf]}\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        if not isinstance(bins_dict, dict):\n",
    "            assert '请传入类似 {\\'D157\\': [-999, 1.0, 2.0, 3.0, 5.0, inf]}'\n",
    "\n",
    "        data_with_bins = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(pd.cut)(df[col], bins=bins, right=False, retbins=True) for col, bins in bins_dict.items())\n",
    "        data_bin = pd.DataFrame([i[0].astype(str) for i in data_with_bins]).T\n",
    "        b_dict = dict([(i[0].name, i[1].tolist()) for i in data_with_bins])\n",
    "        if not operator.eq(bins_dict, b_dict):\n",
    "            assert '传入的分箱和应用后的分箱不对等，请联系开发者'\n",
    "\n",
    "        return data_bin\n",
    "\n",
    "\n",
    "# =============================\n",
    "\n",
    "class ModelTune():\n",
    "    def __init__(self):\n",
    "        self.base_model = None\n",
    "        self.best_model = None\n",
    "        self.model_params = None\n",
    "        self.loss = np.inf\n",
    "        self.metrics = None\n",
    "        self.default_params = None\n",
    "        self.int_params = None\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.best_model\n",
    "\n",
    "    def fit(self, train_data=(), test_data=()\n",
    "            , init_points=10, iterations=15):\n",
    "\n",
    "        X_train, y_train = train_data\n",
    "        X_test, y_test = test_data\n",
    "\n",
    "        # def loss_fun(train_result, test_result):\n",
    "        #     train_result = train_result * 100\n",
    "        #     test_result = test_result * 100\n",
    "        #     if train_result == test_result:\n",
    "        #         return test_result\n",
    "        #\n",
    "        #     import math\n",
    "        #     return test_result - math.log(abs(test_result - train_result))\n",
    "\n",
    "        def loss_fun(train_result, test_result):\n",
    "            train_result = train_result * 100\n",
    "            test_result = test_result * 100\n",
    "\n",
    "            return test_result - 2 ** abs(test_result - train_result)\n",
    "\n",
    "        # def loss_fun(train_result, test_result):\n",
    "        #     train_result = train_result * 100\n",
    "        #     test_result = test_result * 100\n",
    "        #\n",
    "        #     return train_result - 2 ** abs(train_result - test_result)\n",
    "\n",
    "        def obj_fun(**params):\n",
    "            for param in self.int_params:\n",
    "                params[param] = int(round(params[param]))\n",
    "\n",
    "            model = self.base_model(**params, **self.default_params)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            pred_test = model.predict_proba(X_test)[:, 1]\n",
    "            pred_train = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "            test_auc = get_roc_auc_score(y_test, pred_test)\n",
    "            train_auc = get_roc_auc_score(y_train, pred_train)\n",
    "            print('test_auc is : ', test_auc)\n",
    "            print('train_auc is : ', train_auc)\n",
    "\n",
    "            test_ks = get_ks(y_test, pred_test)\n",
    "            train_ks = get_ks(y_train, pred_train)\n",
    "\n",
    "            max_result = loss_fun(train_auc, test_auc)\n",
    "            # max_result = loss_fun(train_ks, test_ks) * 2 + loss_fun(train_auc, test_auc)\n",
    "\n",
    "            loss = 1 - max_result\n",
    "            if loss < self.loss:\n",
    "                self.loss = loss\n",
    "                self.best_model = model\n",
    "                print('best model result is {}'.format(1 - loss))\n",
    "                print('best model result is : ')\n",
    "                print(self.best_model.get_params())\n",
    "            print('current obj_fun result is : ', max_result)\n",
    "\n",
    "            return max_result\n",
    "\n",
    "        params_optimizer = BayesianOptimization(obj_fun, self.model_params, random_state=1)\n",
    "        print('params_optimizer is : ', params_optimizer.space.keys)\n",
    "\n",
    "        print('begain optimizer params!!!')\n",
    "        start = time.time()\n",
    "        params_optimizer.maximize(init_points=init_points, n_iter=iterations, acq='ei', xi=0.0)\n",
    "        # params_optimizer.maximize(init_points=init_points, n_iter=iterations, acq='ucb', xi=0.0, alpha=1e-6)\n",
    "        end = time.time()\n",
    "        print('optimizer params over!!! 共耗时{} 分钟'.format((end - start) / 60))\n",
    "        print('the best params is : {}'.format(params_optimizer.max['params']))\n",
    "        print('Maximum xgb value is : {}'.format(params_optimizer.max['target']))\n",
    "\n",
    "\n",
    "class ClassifierModel(ModelTune):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = ['auc', 'ks']\n",
    "\n",
    "\n",
    "class RegressorModel(ModelTune):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = ['r2', 'rmse']\n",
    "\n",
    "\n",
    "class XGBClassifierTuner(ClassifierModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 先执行父类\n",
    "\n",
    "        self.base_model = XGBClassifier\n",
    "        self.model_params = {\n",
    "            'min_child_weight': (1, 300),\n",
    "            'max_depth': (2, 10),\n",
    "            'n_estimators': (50, 300),\n",
    "            'learning_rate': (0.01, 0.2),\n",
    "            'subsample': (0.4, 1.0),\n",
    "            'colsample_bytree': (0.3, 1.0),\n",
    "            'gamma': (0, 2.0),\n",
    "            'reg_alpha': (0, 2.0),\n",
    "            'reg_lambda': (0, 2.0),\n",
    "            # 'max_delta_step': (0, 10)\n",
    "        }\n",
    "\n",
    "        self.default_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'n_jobs': -1,\n",
    "            'nthread': -1\n",
    "        }\n",
    "\n",
    "        self.int_params = ['max_depth', 'n_estimators']\n",
    "\n",
    "\n",
    "class LGBClassifierTuner(ClassifierModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 先执行父类\n",
    "\n",
    "        self.base_model = LGBMClassifier\n",
    "        self.model_params = {\n",
    "            'max_depth': (500, 1500),\n",
    "            'num_leaves': (200, 800),\n",
    "            'min_data_in_leaf': (50, 250),\n",
    "            'n_estimators': (750, 1800),\n",
    "            'min_child_weight': (0.01, 0.05),\n",
    "            'bagging_fraction': (0.2, 1.0),\n",
    "            'feature_fraction': (0.15, 1.0),\n",
    "            'learning_rate': (0.005, 0.01),\n",
    "            'reg_alpha': (0.2, 0.6),\n",
    "            'reg_lambda': (0.25, 1.0)\n",
    "        }\n",
    "\n",
    "        self.default_params = {\n",
    "            'objective': 'binary',\n",
    "            # 'max_depth': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'bagging_seed': 11,\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'random_state': 47,\n",
    "            'num_threads': -1\n",
    "        }\n",
    "\n",
    "        self.int_params = ['max_depth', 'num_leaves', 'min_data_in_leaf', 'n_estimators']\n",
    "\n",
    "\n",
    "classifiers_dic = {\n",
    "    # 'logistic_regression': LogisticRegressionTuner,\n",
    "    # 'random_forest': RandomForestClassifierTuner,\n",
    "    'xgboost': XGBClassifierTuner,\n",
    "    # 'lgb': LGBClassifierTuner\n",
    "}\n",
    "\n",
    "\n",
    "def classifiers_model(models=[], metrics=[], train_data=(), test_data=()\n",
    "                      , init_points=10, iterations=25, verbose=1):\n",
    "    if type(models) != list:\n",
    "        raise AttributeError('Argument `models` must be a list, ',\n",
    "                             'but given {}'.format(type(models)))\n",
    "    if len(models) == 0:\n",
    "        models = list(classifiers_dic.keys())\n",
    "    classifiers = []\n",
    "    for model in models:\n",
    "        if model in classifiers_dic:\n",
    "            classifiers.append(classifiers_dic[model])\n",
    "    loss = float('inf')\n",
    "    _model = None\n",
    "    for classifier in classifiers:\n",
    "        if verbose:\n",
    "            print(\"Optimizing {}...\".format(classifier()))\n",
    "        _model = classifier()\n",
    "        _model.fit(train_data=train_data,\n",
    "                   test_data=test_data\n",
    "                   , init_points=init_points, iterations=iterations)\n",
    "\n",
    "    return _model.get_model()\n",
    "\n",
    "\n",
    "def sigle_feature_fit_model(train_data=(), test_data=(), is_noise=False, is_only_return_auc=True):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    l = []\n",
    "    for i in X_train:\n",
    "        # xclf = xgb.XGBClassifier(colsample_bytree=0.3, seed=123, random_state=1234)\n",
    "        xclf = xgb.XGBClassifier(**{\n",
    "            'objective': 'binary:logistic',\n",
    "            'n_jobs': -1,\n",
    "            'nthread': -1\n",
    "        })\n",
    "        xclf.fit(X_train[[i]], y_train)\n",
    "        pred_y_train = xclf.predict_proba(X_train[[i]])[:, 1]\n",
    "        auc = get_roc_auc_score(y_train, pred_y_train)\n",
    "        ks = get_ks(y_train, pred_y_train)\n",
    "        l.append((i, auc, ks))\n",
    "\n",
    "    var_auc_ks_df = pd.DataFrame(l, columns=['features', 'auc', 'ks'])\n",
    "\n",
    "    var_auc_ks_df = var_auc_ks_df[var_auc_ks_df['auc'] > 0.51]\n",
    "    # var_auc_ks_df = var_auc_ks_df[var_auc_ks_df['auc'] > 0.1]\n",
    "    # var_auc_ks_df.sort_values(by=['auc', 'ks'], ascending=False, inplace=True)\n",
    "    # print(var_auc_ks_df)\n",
    "    if is_noise:\n",
    "        var_auc_ks_df.to_excel('xgb_not_del_corr_var_auc_ks_df.xlsx')\n",
    "    else:\n",
    "        var_auc_ks_df.to_excel('xgb_var_auc_ks_df.xlsx')\n",
    "    if is_only_return_auc:\n",
    "        return var_auc_ks_df.drop(columns='ks')\n",
    "    else:\n",
    "        return var_auc_ks_df\n",
    "\n",
    "\n",
    "def sigle_feature_auc_ks(train_data=(), test_data=(), is_noise=False, is_only_return_auc=True):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    l = []\n",
    "    for i in X_train:\n",
    "        auc = get_roc_auc_score(y_train, X_train[i])\n",
    "        ks = get_ks(y_train, X_train[i])\n",
    "        l.append((i, auc, ks))\n",
    "\n",
    "    var_auc_ks_df = pd.DataFrame(l, columns=['features', 'auc', 'ks'])\n",
    "    # var_auc_ks_df.sort_values(by=['auc', 'ks'], ascending=False, inplace=True)\n",
    "    # print(var_auc_ks_df)\n",
    "    # if is_noise:\n",
    "    #     var_auc_ks_df.to_excel('process_after_data/xgb_not_del_corr_var_auc_ks_df_zhijie.xlsx')\n",
    "    # else:\n",
    "    #     var_auc_ks_df.to_excel('process_after_data/xgb_var_auc_ks_df_zhijie.xlsx')\n",
    "    if is_only_return_auc:\n",
    "        return var_auc_ks_df.drop(columns='ks')\n",
    "    else:\n",
    "        return var_auc_ks_df\n",
    "\n",
    "\n",
    "def change_col_subsample_fit_model(train_data=(), test_data=()):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    colsample_bytree = [i / 10 for i in range(3, 11)]\n",
    "    subsample = [i / 10 for i in range(3, 11)]\n",
    "\n",
    "    imp_l = []\n",
    "    for i in range(8):\n",
    "        params = {\n",
    "            'min_child_weight': 10,\n",
    "            'subsample': subsample[i],\n",
    "            'colsample_bytree': colsample_bytree[i],\n",
    "            'objective': 'binary:logistic',\n",
    "            'n_jobs': -1,\n",
    "            'nthread': -1\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        imp = pd.DataFrame(list(model.get_booster().get_score().items()),\n",
    "                           columns=['features', 'feature_importances']).set_index('features')\n",
    "        imp_l.append(imp)\n",
    "\n",
    "        pred_y_train = model.predict_proba(X_train)[:, 1]\n",
    "        pred_y_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        print('subsample is {} and colsample_bytree is {} model result is : '.format(subsample[i], colsample_bytree[i]))\n",
    "        print('xgb train auc is ：', get_roc_auc_score(y_train, pred_y_train))\n",
    "        print('xgb train ks is ：', get_ks(y_train, pred_y_train))\n",
    "        print('xgb test auc is ：', get_roc_auc_score(y_test, pred_y_test))\n",
    "        print('xgb test ks is ：', get_ks(y_test, pred_y_test))\n",
    "\n",
    "    imp_df = pd.concat(imp_l, axis=1)\n",
    "    imp_df['mean_imp'] = imp_df.mean(axis=1)\n",
    "    # imp_df.to_excel('imp_df_all_mean.xlsx')\n",
    "    imp_df.drop(columns=['feature_importances'], inplace=True)\n",
    "    # imp_df.to_excel('imp_df_mean.xlsx')\n",
    "    imp_df.reset_index(inplace=True)\n",
    "    return imp_df\n",
    "\n",
    "\n",
    "def kfold_xgb_model(train_data=(), is_noise=False, cv=StratifiedKFold(10, shuffle=True)):\n",
    "    X, y = train_data\n",
    "    cv_data = cv.split(X, y)\n",
    "\n",
    "    train_auc_l = []\n",
    "    valid_auc_l = []\n",
    "    feature_imp = []\n",
    "    for fold_num, (train_i, valid_i) in enumerate(cv_data):\n",
    "        X_train, y_train = X.iloc[train_i], y.iloc[train_i]\n",
    "        X_valid, y_valid = X.iloc[valid_i], y.iloc[valid_i]\n",
    "\n",
    "        # print(X.shape)\n",
    "        # print(X_train.shape)\n",
    "        # print(X_valid.shape)\n",
    "\n",
    "        model = xgb.XGBClassifier(**{\n",
    "            'objective': 'binary:logistic',\n",
    "            'n_jobs': -1,\n",
    "            'nthread': -1\n",
    "        })\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        pred_y_train = model.predict_proba(X_train)[:, 1]\n",
    "        pred_y_valid = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        train_auc = get_roc_auc_score(y_train, pred_y_train)\n",
    "        valid_auc = get_roc_auc_score(y_valid, pred_y_valid)\n",
    "\n",
    "        feature_importance = pd.DataFrame(list(model.get_booster().get_score().items()),\n",
    "                                          columns=['features', 'feature_importances'])\n",
    "        # feature_importance.sort_values(by='feature_importances', ascending=False,\n",
    "        #                                inplace=True)\n",
    "        feature_importance.set_index('features', inplace=True)\n",
    "        # feature_importance.reset_index(inplace=True)\n",
    "        # print(feature_importance)\n",
    "        feature_imp.append(feature_importance)\n",
    "\n",
    "        print('Fold {} , train auc is {}, valid_auc is {}'.format(fold_num, train_auc, valid_auc))\n",
    "\n",
    "        train_auc_l.append(train_auc)\n",
    "        valid_auc_l.append(valid_auc)\n",
    "\n",
    "    train_mean_auc = np.array(train_auc_l).mean()\n",
    "    valid_mean_auc = np.array(valid_auc_l).mean()\n",
    "    feature_imp_all = pd.concat(feature_imp, axis=1)\n",
    "    print('train_mean_auc is : {}'.format(train_mean_auc))\n",
    "    print('valid_mean_auc is : {}'.format(valid_mean_auc))\n",
    "    # print('feature_imp_all is : {}'.format(feature_imp_all))\n",
    "    # feature_imp_all.to_excel('process_after_data/feature_imp_all.xlsx')\n",
    "\n",
    "    feature_imp_all['mean_imp'] = feature_imp_all.mean(axis=1)\n",
    "    # feature_imp_all.to_excel('process_after_data/feature_imp_all_mean.xlsx')\n",
    "    feature_imp_all.drop(columns='feature_importances', inplace=True)\n",
    "    # feature_imp_all.index.name = 'features'\n",
    "    feature_imp_all.reset_index(inplace=True)\n",
    "    return feature_imp_all\n",
    "\n",
    "\n",
    "def unpack_tuple(x):\n",
    "    if len(x) == 1:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def drop_corr(frame, by='auc', threshold=0.95, return_drop=False):\n",
    "    if not isinstance(by, (str, pd.Series)):\n",
    "\n",
    "        if isinstance(by, pd.DataFrame):\n",
    "            by = pd.Series(by.iloc[:, 1].values, index=by.iloc[:, 0].values)\n",
    "            # by = pd.Series(by.iloc[:, 1].values, index=frame.columns)\n",
    "        else:\n",
    "            by = pd.Series(by, index=frame.columns)\n",
    "\n",
    "    # 给重要性排下序\n",
    "    by.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    # df = frame.copy()\n",
    "\n",
    "    by.index = by.index.astype(type(frame.columns.to_list()[0]))\n",
    "    df_corr = frame[by.index.to_list()].fillna(-999).corr().abs()\n",
    "\n",
    "    ix, cn = np.where(np.triu(df_corr.values, 1) > threshold)\n",
    "\n",
    "    del_all = []\n",
    "\n",
    "    if len(ix):\n",
    "\n",
    "        for i in df_corr:\n",
    "\n",
    "            if i not in del_all:\n",
    "                # 找出与当前特征的相关性大于域值的特征\n",
    "                del_tmp = df_corr[i][(df_corr[i] > threshold) & (df_corr[i] != 1)].index.to_list()\n",
    "\n",
    "                # 比较当前特征与需要删除的特征的特征重要性\n",
    "                if del_tmp:\n",
    "                    by_tmp = by.loc[del_tmp]\n",
    "                    del_l = by_tmp[by_tmp <= by.loc[i]].index.to_list()\n",
    "                    del_all.extend(del_l)\n",
    "\n",
    "    del_f = list(set(del_all))\n",
    "\n",
    "    r = frame.drop(columns=del_f)\n",
    "\n",
    "    res = (r,)\n",
    "    if return_drop:\n",
    "        res += (del_f,)\n",
    "\n",
    "    return unpack_tuple(res)\n",
    "\n",
    "\n",
    "def forward_corr_delete(df, col_list):\n",
    "    corr_list = []\n",
    "    corr_list.append(col_list[0])\n",
    "    delete_col = []\n",
    "    # 根据特征重要性的大小进行遍历\n",
    "    for col in col_list[1:]:\n",
    "        corr_list.append(col)\n",
    "        corr = df.loc[:, corr_list].corr()\n",
    "        corr_tup = [(x, y) for x, y in zip(corr[col].index, corr[col].values)]\n",
    "        corr_value = [y for x, y in corr_tup if x != col]\n",
    "        # 若出现相关系数大于0。65，则将该特征剔除\n",
    "        if len([x for x in corr_value if abs(x) >= 0.5]) > 0:\n",
    "            delete_col.append(col)\n",
    "\n",
    "    select_corr_col = [x for x in col_list if x not in delete_col]\n",
    "    return select_corr_col\n",
    "\n",
    "\n",
    "def xgb_model(train_data=(), test_data=(), is_noise=False):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    params = {\n",
    "        'min_child_weight': 10,\n",
    "        'subsample': 0.5,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'objective': 'binary:logistic',\n",
    "        'n_jobs': -1,\n",
    "        'nthread': -1\n",
    "    }\n",
    "    xclf = xgb.XGBClassifier(**params)\n",
    "    # xclf = xgb.XGBClassifier()\n",
    "    xclf.fit(X_train, y_train)\n",
    "\n",
    "    print('===============xgb 不同的重要性===============')\n",
    "    l = []\n",
    "    print('weight')\n",
    "    importance_type = 'weight'\n",
    "    feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "                                      columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # feature_importance.set_index('features', inplace=True)\n",
    "    feature_importance.reset_index(inplace=True)\n",
    "    print(feature_importance)\n",
    "    l.append(feature_importance)\n",
    "\n",
    "    print('gain')\n",
    "    importance_type = 'gain'\n",
    "    feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "                                      columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # feature_importance.set_index('features', inplace=True)\n",
    "    feature_importance.reset_index(inplace=True)\n",
    "    # print(feature_importance)\n",
    "    l.append(feature_importance)\n",
    "\n",
    "    print('cover')\n",
    "    importance_type = 'cover'\n",
    "    feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "                                      columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # feature_importance.set_index('features', inplace=True)\n",
    "    feature_importance.reset_index(inplace=True)\n",
    "    # print(feature_importance)\n",
    "    l.append(feature_importance)\n",
    "\n",
    "    # print('total_gain')\n",
    "    # importance_type = 'total_gain'\n",
    "    # feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "    #                                   columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    # feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # # feature_importance.set_index('features', inplace=True)\n",
    "    # feature_importance.reset_index(inplace=True)\n",
    "    # print(feature_importance)\n",
    "    # l.append(feature_importance)\n",
    "    #\n",
    "    # print('total_cover')\n",
    "    # importance_type = 'total_cover'\n",
    "    # feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "    #                                   columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    # feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # # feature_importance.set_index('features', inplace=True)\n",
    "    # feature_importance.reset_index(inplace=True)\n",
    "    # print(feature_importance)\n",
    "    # l.append(feature_importance)\n",
    "\n",
    "    five_importance = pd.concat(l, axis=1)\n",
    "    if is_noise:\n",
    "        five_importance.to_excel('xgb_not_del_corr_five_importance.xlsx')\n",
    "    else:\n",
    "        five_importance.to_excel('xgb_five_importance.xlsx')\n",
    "\n",
    "    print(np.mean(\n",
    "        cross_val_score(estimator=xclf, X=X_train, y=y_train, scoring='accuracy',\n",
    "                        cv=StratifiedKFold(5, random_state=123))))\n",
    "\n",
    "    pred_y_train = xclf.predict_proba(X_train)[:, 1]\n",
    "    pred_y_test = xclf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print('xgb train auc is ：', get_roc_auc_score(y_train, pred_y_train))\n",
    "    print('xgb train ks is ：', get_ks(y_train, pred_y_train))\n",
    "    print('xgb test auc is ：', get_roc_auc_score(y_test, pred_y_test))\n",
    "    print('xgb test ks is ：', get_ks(y_test, pred_y_test))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # =========================step 1 相关配置=========================\n",
    "    log.info('step 1 相关配置')\n",
    "    feature_type = 'lhpdat'  # 什么数据\n",
    "    cust_id = 'apply_no'  # 主键\n",
    "    target = 'target'  # 目标变量\n",
    "    data_type = 'type'  # 区分数据集变量\n",
    "    apply_time = 'apply_time'  # 时间\n",
    "\n",
    "    client = 'lhp09'\n",
    "    batch = 'p23'\n",
    "\n",
    "    to_model_var_num = 30  # 不限制的话修改为None\n",
    "    is_model_data_to_woe = False  # 喂入模型的数据是否需要转化为woe值，False不需要，即原始数据入模型\n",
    "    fillna_value = -999999  # 缺失值填充的值\n",
    "\n",
    "    # 阈值配置\n",
    "    exclude_cols = [apply_time, cust_id, target, data_type, 'apply_month']\n",
    "    feature_missing_threshould = 0.95  # 缺失率大于等于该阈值的变量剔除\n",
    "\n",
    "    # 需要删除的变量\n",
    "    need_drop_cols = ['applthst_loan_amount', 'tzre_report_info_report_no', 'xy_black_trade_no', 'tzre_id',\n",
    "                      'xy_black_version', 'tzre_version', 'tzre_bi_phone_number']\n",
    "\n",
    "    # 用于训练模型的数据\n",
    "    label_encoder_dict = {}\n",
    "    to_model_data_path = '/Users/ryanzheng/PycharmProjects/data_to_treemodel_v1/to_model_data/lhp_amount_rule_jm.csv'\n",
    "\n",
    "    # =========================后续代码基本可以不用动=========================\n",
    "\n",
    "    # 基本不用动\n",
    "    project_name = '{}{}'.format(client, batch)\n",
    "    client_batch = '{}{}'.format(client, batch)\n",
    "    project_dir = 'model_result_data/{}/{}/'.format(client, batch)\n",
    "    output_dir = '{}model/{}/'.format(project_dir, feature_type)\n",
    "\n",
    "    os.makedirs(project_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(project_dir + 'data/score/', exist_ok=True)\n",
    "    os.makedirs(project_dir + 'data/xgb_score/', exist_ok=True)\n",
    "    # 基本不用动\n",
    "    # =========================相关配置=========================\n",
    "\n",
    "    # In[38]:\n",
    "\n",
    "    # =========================step 2 读取数据集=========================\n",
    "    log.info('step 2 开始读取数据集')\n",
    "    # 读取宽表数据\n",
    "    log.info('读取样本&特征数据集：{}|{}|{}为样本数据，其他为特征数据'.format(cust_id, apply_time, target))\n",
    "    all_data = pd.read_csv(to_model_data_path)\n",
    "    all_data[target] = np.where(all_data[target] <= 200, 1, 0)\n",
    "\n",
    "    # drop_cols = ['xy_black_version', 'tzre_version']\n",
    "    # all_data.drop(columns=drop_cols, axis=1, inplace=True)\n",
    "    all_data.drop(need_drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    all_data.set_index(cust_id, inplace=True)\n",
    "    selected_features = all_data.columns.format()\n",
    "    selected_features = list(set(selected_features) - set(exclude_cols))\n",
    "    log.info('特征的个数：{}'.format(len(selected_features)))\n",
    "\n",
    "    # =========================读取字典进行重命名=========================\n",
    "    #     ##读取字典进行重命名\n",
    "    #     fea_dict_df = pd.read_excel('/home/marketingscore/ryanzheng/fit_model_project/新特征数据字典v3.xlsx')\n",
    "    #     fea_dict = fea_dict_df[['feature_code','feature_id']].set_index('feature_code')['feature_id'].to_dict()\n",
    "    #     all_data.rename(columns=fea_dict, inplace=True)\n",
    "\n",
    "    #     selected_features = all_data.columns.format()\n",
    "    #     selected_features = list(set(selected_features) - set(exclude_cols))\n",
    "    # #     if exclude_vars:\n",
    "    # #         selected_features = list(set(selected_features) - set(exclude_vars))\n",
    "\n",
    "    #     ##仅使用数据字典中有的变量\n",
    "    #     fea_dict_df_list = fea_dict_df['feature_id'].tolist()\n",
    "    #     selected_features = list(set(selected_features).intersection(set(fea_dict_df_list)))\n",
    "    #     print(len(selected_features))\n",
    "    #     ##仅使用数据字典中有的变量\n",
    "\n",
    "    # =========================读取字典进行重命名=========================\n",
    "\n",
    "    # 删除特征全为空的样本量\n",
    "    log.info('删除特征全为空的样本量')\n",
    "    print('删除特征全为空的样本之前的数据集行列：', all_data.shape)\n",
    "    all_data.dropna(subset=selected_features, how='all', inplace=True)\n",
    "    print('删除特征全为空的样本之后的数据集行列：', all_data.shape)\n",
    "\n",
    "    log.info('样本数据集情况：')\n",
    "    log.info(all_data[target].value_counts())\n",
    "    # =========================读取数据集=========================\n",
    "\n",
    "    log.info('EDA，整体数据探索性数据分析')\n",
    "    # all_data_eda = detect(all_data)\n",
    "    # all_data_eda.to_excel('{}{}_{}_all_data_eda.xlsx'.format(\n",
    "    #     output_dir, project_name, feature_type))\n",
    "\n",
    "    # =========================step 3 划分训练集和测试集=========================\n",
    "    log.info('step 3 划分训练集和测试集')\n",
    "    if data_type not in all_data.columns:\n",
    "        df_sample = all_data[[target, apply_time]]\n",
    "        df_sample.reset_index(inplace=True)\n",
    "\n",
    "        # 随机切分train、test\n",
    "        df_sample = split_data_type(df_sample, key_col=cust_id, target=target, apply_time=apply_time, test_size=0.25)\n",
    "        df_sample.to_csv(project_dir + 'data/{}_split.csv'.format(client_batch), index=False)\n",
    "\n",
    "        #         #按时间切分\n",
    "        #         df_oot = df_sample[df_sample['apply_time']>= '2020-04-01']\n",
    "        #         X_train = df_sample[df_sample['apply_time']<= '2020-02-01']\n",
    "        #         X_test = df_sample[(df_sample['apply_time']> '2020-02-01') & (df_sample['apply_time']< '2020-04-01')]\n",
    "\n",
    "        #         df_sample.loc[df_oot.index,'type'] = 'oot'\n",
    "        #         df_sample.loc[X_train.index,'type'] = 'train'\n",
    "        #         df_sample.loc[X_test.index,'type'] = 'test'\n",
    "\n",
    "        df_sample.to_csv(project_dir + 'data/{}_split.csv'.format(client_batch), index=False)\n",
    "        df_sample.set_index(cust_id, inplace=True)\n",
    "        print(df_sample['type'].value_counts())\n",
    "\n",
    "    # In[39]:\n",
    "\n",
    "    # 将数据集类别和数据集合并\n",
    "    # df_sample = all_data[[target, apply_time, data_type]]\n",
    "    all_data = pd.merge(df_sample[['type']], all_data, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    log.info('分开训练集和测试集为两个df')\n",
    "    train_data = all_data[all_data['type'] == 'train']\n",
    "    # test_data = all_data[all_data['type'] == 'test']\n",
    "\n",
    "    log.info('EDA，训练集探索性数据分析')\n",
    "    # detect(train_data).to_excel('{}{}_{}_train_data_eda.xlsx'.format(\n",
    "    #     output_dir, project_name, feature_type))\n",
    "    #     detect(test_data).to_excel('{}{}_{}_test_data_eda.xlsx'.format(\n",
    "    #         output_dir, project_name, feature_type))\n",
    "\n",
    "    # =========================step 4 初筛=========================\n",
    "    log.info('step 4 变量初筛')\n",
    "    # selected_features = train_data_eda[train_data_eda['missing_q'] <= 0.95].index.to_list()\n",
    "    print('删除缺失率前变量数量：', len(selected_features))\n",
    "    selected_features = filter_miss(train_data[selected_features], miss_threshold=feature_missing_threshould)\n",
    "    print('删除缺失率后变量数量：', len(selected_features))\n",
    "    train_data = train_data[selected_features + [target]]\n",
    "    # test_data = test_data[selected_features + [target]]\n",
    "    # =========================初筛=========================\n",
    "\n",
    "    # =========================step 5 数据处理=========================\n",
    "    log.info('step 5 数据woe处理')\n",
    "\n",
    "    # 离散变量数据处理\n",
    "    # selected_features = list(set(selected_features) - set(exclude_cols))\n",
    "    continuous_cols, category_cols, date_cols = select_features_dtypes(train_data[selected_features])\n",
    "\n",
    "    train_data.loc[:, continuous_cols] = train_data.loc[:, continuous_cols].fillna(fillna_value)\n",
    "    # test_data.loc[:, continuous_cols] = test_data.loc[:, continuous_cols].fillna(fillna_value)\n",
    "    all_data.loc[:, continuous_cols] = all_data.loc[:, continuous_cols].fillna(fillna_value)\n",
    "    # data.loc[:, continuous_cols] = data.loc[:, continuous_cols].fillna(-999)\n",
    "\n",
    "    # =========================labelencode=========================\n",
    "    #     def category_to_labelencoder(data, labelencoder=[]):\n",
    "    #         label_encoder_dict = {}\n",
    "    #         le = LabelEncoder()\n",
    "    #         for col in labelencoder:\n",
    "    #             print('{} in process!!!'.format(col))\n",
    "    #             data[col] = le.fit_transform(data[col].values)\n",
    "    #             number = [i for i in range(0, len(le.classes_))]\n",
    "    #             key = list(le.inverse_transform(number))\n",
    "    #             label_encoder_dict[col] = dict(zip(key, number))\n",
    "    #         return label_encoder_dict\n",
    "\n",
    "    #     def category_to_labelencoder_apply(data, labelencoder_dict={}):\n",
    "    #         for col, mapping in labelencoder_dict.items():\n",
    "    #             print('{} in process!!!'.format(col))\n",
    "    #             data[col] = data[col].map(mapping).fillna(-1)\n",
    "    #             data[col] = data[col].astype(int)\n",
    "\n",
    "    #     if category_cols:\n",
    "    #         train_data.loc[:, category_cols] = train_data.loc[:, category_cols].fillna('-1007')\n",
    "    #         all_data.loc[:, category_cols] = all_data.loc[:, category_cols].fillna('-1007')\n",
    "    #         label_encoder_dict = category_to_labelencoder(train_data, category_cols)\n",
    "    #         category_to_labelencoder_apply(all_data, label_encoder_dict)\n",
    "\n",
    "    # =========================labelencode=========================\n",
    "\n",
    "    if category_cols and not label_encoder_dict:\n",
    "        log.info('step 5.1 类别变量数据处理')\n",
    "        # train_data.loc[:, category_cols] = train_data.loc[:, category_cols].fillna('miss')\n",
    "        # test_data.loc[:, category_cols] = test_data.loc[:, category_cols].fillna('miss')\n",
    "\n",
    "        var_value_woe = category_2_woe(train_data, category_cols, target=target)\n",
    "        category_2_woe_save(var_value_woe, '{}'.format(output_dir))\n",
    "        # var_value_woe = category_2_woe_load('{}'.format(output_dir))\n",
    "        train_data = WoeTransformer().transform(train_data, var_value_woe)\n",
    "        # test_data = WoeTransformer().transform(test_data, var_value_woe)\n",
    "        all_data = WoeTransformer().transform(all_data, var_value_woe)\n",
    "\n",
    "    # 离散变量数据处理\n",
    "\n",
    "    # In[40]:\n",
    "\n",
    "    if is_model_data_to_woe:\n",
    "        log.info('将箱子转woe')\n",
    "        log.info('============入模数据需要转化为woe值===========')\n",
    "        #         train_data_to_model = WoeTransformer().transform(train_data_bin, fb.get_var_bin_woe())\n",
    "        #         test_data_to_model = WoeTransformer().transform(test_data_bin, fb.get_var_bin_woe())\n",
    "        # all_data_to_model = WoeTransformer().transform(all_data_bin, fb.get_var_bin_woe())\n",
    "    else:\n",
    "        log.info('============入模数据不需要转化为woe值===========')\n",
    "        #         train_data_to_model = train_data.copy()\n",
    "        #         test_data_to_model = test_data.copy()\n",
    "        all_data_to_model = all_data.copy()\n",
    "\n",
    "\n",
    "    # In[41]:\n",
    "\n",
    "    def statistics_model_result(all_data=pd.DataFrame()):\n",
    "        # ===========================step 6 统计=================================\n",
    "        all_data['score'] = all_data[feature_type].map(lambda v: to_score(v))\n",
    "        log.info('模型相关结果统计！！！')\n",
    "        df_splitted_type_auc_ks = all_data.groupby(data_type).apply(\n",
    "            lambda df: pd.Series({'auc': get_roc_auc_score(df[target], df['score']),\n",
    "                                  'ks': get_ks(df[target], df['score'])}))\n",
    "        df_splitted_type_auc_ks = df_splitted_type_auc_ks.reindex(['train', 'test', 'oot', 'cv'])\n",
    "\n",
    "        log.info('模型效果：')\n",
    "        print(df_splitted_type_auc_ks)\n",
    "\n",
    "        all_data['month'] = all_data[apply_time].map(lambda s: s[:7])\n",
    "        df_monthly_auc_ks = all_data.groupby('month').apply(\n",
    "            lambda df: pd.Series({'auc': get_roc_auc_score(df[target], df['score']),\n",
    "                                  'ks': get_ks(df[target], df['score'])}))\n",
    "        del all_data['month']\n",
    "        log.info('不同月份的模型效果：')\n",
    "        print(df_monthly_auc_ks)\n",
    "\n",
    "        df_desc = all_data[[feature_type, 'score']].describe()\n",
    "        df_desc.loc['coverage'] = df_desc.loc['count'] / all_data.shape[0]\n",
    "        log.info('分数describe')\n",
    "        print(df_desc)\n",
    "\n",
    "        all_data[data_type] = all_data[data_type].map(lambda s: s.lower())\n",
    "        all_data['client_batch'] = client_batch\n",
    "        # df_psi,df_psi_details = psi_statis(all_data, splitted_types=['train','test','oot'], scores=[feature_type])\n",
    "        df_psi, df_psi_details = psi_statis(all_data, splitted_types=['train', 'test'], scores=[feature_type])\n",
    "        del all_data['client_batch']\n",
    "        log.info('模型psi：')\n",
    "        print(df_psi[['train_test_psi']])\n",
    "        # log.info(df_psi[['train_test_psi','train_oot_psi']])\n",
    "\n",
    "        df_output_statis = df_splitted_type_auc_ks.reset_index()\n",
    "        df_output_statis['feature'] = feature_type\n",
    "        df_output_statis['project_name'] = project_name\n",
    "        df_output_statis['client_batch'] = client_batch\n",
    "        df_output_statis = df_output_statis.pivot_table(\n",
    "            index=['project_name', 'client_batch', 'feature'],\n",
    "            columns=data_type,\n",
    "            values=['auc', 'ks'])\n",
    "        df_output_statis.columns = ['_'.join(reversed(x)) for x in df_output_statis.columns]\n",
    "        df_output_statis['feature_cnt'] = len(selected_features)\n",
    "        df_output_statis['n_estimators'] = model.get_params()['n_estimators']\n",
    "\n",
    "        log.info('统计结束')\n",
    "        return df_output_statis\n",
    "        # ===========================统计=================================\n",
    "\n",
    "\n",
    "    # In[42]:\n",
    "\n",
    "    # =========================step 6 训练模型=========================\n",
    "    X_all, y_all, X_train, y_train, X_test, y_test, X_oot, y_oot = get_splitted_data(\n",
    "        all_data_to_model, target=target, selected_features=selected_features)\n",
    "\n",
    "    print('整体数据集大小：', X_all.shape)\n",
    "    print('训练集大小：', X_train.shape)\n",
    "    print('测试集大小：', X_test.shape)\n",
    "    if X_oot is None:\n",
    "        print('无oot数据集')\n",
    "    else:\n",
    "        print('oot集大小：', X_oot.shape)\n",
    "\n",
    "    pd.Series(X_test.index).to_csv('{}{}_{}_X_test_key_{}.csv'.format(\n",
    "        output_dir, project_name, feature_type, cust_id), header=cust_id, index=False)\n",
    "\n",
    "    log.info('step 6 开始训练模型')\n",
    "    start = datetime.now()\n",
    "\n",
    "    log.info('step 6.1 ===筛选变量===')\n",
    "\n",
    "    # ===========================================\n",
    "\n",
    "    log.info('step 6.1 ===筛选变量===10折交叉后，计算变量的平均重要性')\n",
    "    # feature_imp = tree_selection.kfold_xgb_model(train_data=(del_corr_df, y_train))\n",
    "    log.info('筛选前数据集大小：{}'.format(X_train.shape))\n",
    "    feature_imp = change_col_subsample_fit_model(train_data=(X_train, y_train),\n",
    "                                                 test_data=(X_test, y_test))\n",
    "\n",
    "    log.info('将特征重要性持久化')\n",
    "    feature_imp.to_csv('{}{}_{}_xgb_allfeature_mean_imp_df.csv'.format(\n",
    "        output_dir, project_name, feature_type))\n",
    "\n",
    "    log.info('根据10折拟合模型处理后的变量重要性进行变量相关性筛选')\n",
    "    del_corr_df = drop_corr(X_train, by=feature_imp, threshold=0.9)\n",
    "    # del_corr_df = tree_selection.drop_corr(del_corr_df, by=feature_imp, threshold=0.8)\n",
    "    log.info('筛选后数据集大小：{}'.format(del_corr_df.shape))\n",
    "\n",
    "    # ===========================================\n",
    "\n",
    "    selected_features = list(del_corr_df.columns)\n",
    "    log.info('最终入模变量的数量：{}'.format(len(selected_features)))\n",
    "    log.info('最终入模变量：{}'.format(selected_features))\n",
    "\n",
    "    feature_imp = change_col_subsample_fit_model(train_data=(del_corr_df, y_train),\n",
    "                                                 test_data=(X_test[del_corr_df.columns], y_test))\n",
    "\n",
    "    log.info('将待入模特征重要性持久化')\n",
    "    feature_imp.to_csv('{}{}_{}_xgb_tomodel_feature_mean_imp_df.csv'.format(\n",
    "        output_dir, project_name, feature_type))\n",
    "\n",
    "    log.info('贝叶斯进行模型调参')\n",
    "    model = classifiers_model(train_data=(X_train[selected_features], y_train),\n",
    "                              test_data=(X_test[selected_features], y_test),\n",
    "                              init_points=5, iterations=8, verbose=1)\n",
    "    log.info('模型调参完成！！！')\n",
    "    log.info('模型参数：{}'.format(model.get_xgb_params()))\n",
    "    log.info('模型参数：{}'.format(model.get_params()))\n",
    "\n",
    "    df_featurescore = pd.DataFrame(list(model._Booster.get_fscore().items()), columns=['特征名称', '特征权重值']\n",
    "                                   ).sort_values('特征权重值', ascending=False)\n",
    "    df_featurescore.to_csv('{}{}_{}_xgb_featurescore_first.csv'.format(\n",
    "        output_dir, project_name, feature_type), index=False)\n",
    "\n",
    "    end = datetime.now()\n",
    "    log.info('模型训练完成, 使用 {} 秒'.format((end - start).seconds))\n",
    "\n",
    "    # X_all = pd.concat([X_train, X_test])\n",
    "    X_all[feature_type] = model.predict_proba(X_all[selected_features])[:, 1]\n",
    "    all_data = pd.concat([all_data_to_model, X_all[feature_type]], axis=1)\n",
    "\n",
    "    statistics_model_result(all_data=all_data)\n",
    "\n",
    "    # X_all.to_csv('{}{}_{}_X_all.csv'.format(output_dir, project_name, feature_type))\n",
    "    # all_data.to_csv('{}{}_{}_all_data.csv'.format(output_dir, project_name, feature_type))\n",
    "\n",
    "    if to_model_var_num:\n",
    "        start = datetime.now()\n",
    "\n",
    "        print('过滤前{}个特征出来，再次训练'.format(to_model_var_num))\n",
    "        #         importance = model._Booster.get_fscore()\n",
    "        #         importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        #         features_importance = pd.DataFrame()\n",
    "        #         features_importance = features_importance.append(importance, ignore_index=True)\n",
    "        #         features_importance.columns = ['特征名称', '特征权重值']\n",
    "        #         # features_importance.to_csv(\n",
    "        #         #     '{}{}_{}_xgb_features_importance.csv'.format(output_dir, project_name, feature_type))\n",
    "        #         selected_features = features_importance.iloc[:to_model_var_num]['特征名称'].tolist()\n",
    "\n",
    "        selected_features = df_featurescore.iloc[:to_model_var_num]['特征名称'].tolist()\n",
    "\n",
    "        print('过滤后的特征：', selected_features)\n",
    "\n",
    "        X_all, y_all, X_train, y_train, X_test, y_test, X_oot, y_oot = get_splitted_data(\n",
    "            all_data_to_model, target=target, selected_features=selected_features)\n",
    "        print('整体数据集大小：', X_all.shape)\n",
    "        print('训练集大小：', X_train.shape)\n",
    "        print('测试集大小：', X_test.shape)\n",
    "        if X_oot is None:\n",
    "            print('无oot数据集')\n",
    "        else:\n",
    "            print('oot集大小：', X_oot.shape)\n",
    "\n",
    "        # 手动指定调参\n",
    "        # model = xgb.XGBClassifier(**ini_params)\n",
    "        # model.fit(X_train, y_train)\n",
    "\n",
    "        # 贝叶斯调参\n",
    "        log.info('贝叶斯进行模型调参')\n",
    "        model = classifiers_model(train_data=(X_train[selected_features], y_train),\n",
    "                                  test_data=(X_test[selected_features], y_test),\n",
    "                                  init_points=5, iterations=8, verbose=1)\n",
    "        log.info('模型调参完成！！！')\n",
    "        log.info('模型参数：{}'.format(model.get_xgb_params()))\n",
    "        log.info('模型参数：{}'.format(model.get_params()))\n",
    "\n",
    "        end = datetime.now()\n",
    "        log.info('模型训练完成, 使用 {} 秒'.format((end - start).seconds))\n",
    "\n",
    "    # X_all = pd.concat([X_train, X_test])\n",
    "    X_all[feature_type] = model.predict_proba(X_all[selected_features])[:, 1]\n",
    "    all_data = pd.concat([all_data_to_model, X_all[feature_type]], axis=1)\n",
    "\n",
    "    df_output_statis = statistics_model_result(all_data=all_data)\n",
    "\n",
    "    # X_all.to_csv('{}{}_{}_X_all.csv'.format(output_dir, project_name, feature_type))\n",
    "    # all_data.to_csv('{}{}_{}_all_data.csv'.format(output_dir, project_name, feature_type))\n",
    "\n",
    "    # ==========================训练模型=========================\n",
    "\n",
    "    # In[43]:\n",
    "\n",
    "    # ===========================step 7 模型持久化=================================\n",
    "\n",
    "    log.info('模型相关结果持久化')\n",
    "    all_data[feature_type].to_frame().to_csv(\n",
    "        '{}/data/score/{}_{}_score.csv'.format(project_dir, project_name, feature_type))\n",
    "    all_data[feature_type].to_frame().to_csv(\n",
    "        '{}/data/xgb_score/{}_{}_score.csv'.format(project_dir, project_name, feature_type))\n",
    "    all_data[feature_type].to_frame().to_csv('{}{}_{}_score.csv'.format(\n",
    "        output_dir, project_name, feature_type))\n",
    "\n",
    "    joblib.dump(model._Booster, '{}{}_{}_xgb.ml'.format(\n",
    "        output_dir, project_name, feature_type))\n",
    "    json.dump(model.get_params(), open('{}{}_{}_xgb.params'.format(\n",
    "        output_dir, project_name, feature_type), 'w'))\n",
    "\n",
    "    model._Booster.dump_model('{}{}_{}_xgb.txt'.format(output_dir, project_name, feature_type))\n",
    "\n",
    "    df_featurescore = pd.DataFrame(list(model._Booster.get_fscore().items()), columns=['特征名称', '特征权重值']\n",
    "                                   ).sort_values('特征权重值', ascending=False)\n",
    "    df_featurescore.to_csv('{}{}_{}_xgb_featurescore.csv'.format(\n",
    "        output_dir, project_name, feature_type), index=False)\n",
    "\n",
    "    df_corr = X_all.corr()\n",
    "    df_corr.to_csv('{}{}_{}_xgb_corr.csv'.format(\n",
    "        output_dir, project_name, feature_type), index_label='feature')\n",
    "\n",
    "    df_rawdata = all_data[selected_features]\n",
    "    df_rawdata.reset_index(inplace=True)\n",
    "    df_rawdata_col_name = df_rawdata.columns.tolist()\n",
    "    df_rawdata_col_name.insert(len(df_rawdata_col_name) - 1,\n",
    "                               df_rawdata_col_name.pop(df_rawdata_col_name.index(cust_id)))\n",
    "    df_rawdata = df_rawdata[df_rawdata_col_name]\n",
    "    df_rawdata.head(100).to_csv('{}{}_{}_xgb_rawdata.csv'.format(\n",
    "        output_dir, project_name, feature_type), index=False)\n",
    "\n",
    "    df_output_statis.to_csv('{}{}_{}_xgb_output_statis.csv'.format(\n",
    "        output_dir, project_name, feature_type))\n",
    "\n",
    "    os.makedirs(project_dir + 'data/statis/auc_ks', exist_ok=True)\n",
    "    df_output_statis.to_csv('{}data/statis/auc_ks/{}.csv'.format(\n",
    "        project_dir, feature_type))\n",
    "\n",
    "\n",
    "    from model_evaluator import model_save as ms\n",
    "    save_pythonmodel('td06p1_xgb_model_v1.ml',model,script_id=script_id)\n",
    "\n",
    "    log.info('模型相关结果持久化完成')\n",
    "    # ===========================模型持久化=================================\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "    # In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
