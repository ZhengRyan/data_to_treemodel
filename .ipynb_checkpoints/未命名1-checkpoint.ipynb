{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "# !/usr/bin/env python\n",
    "# ! -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    "@File: model_fit_v3.py\n",
    "@Author: RyanZheng\n",
    "@Email: ryan.zhengrp@gmail.com\n",
    "@Created Time on: 2020-07-26\n",
    "\n",
    "y值加密混淆\n",
    "ok 可以跑通\n",
    "'''\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from joblib import Parallel, delayed\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 日志输出\n",
    "class Logger():\n",
    "    # 日志级别关系映射\n",
    "    level_relations = {\n",
    "        \"debug\": logging.DEBUG,\n",
    "        \"info\": logging.INFO,\n",
    "        \"warning\": logging.WARNING,\n",
    "        \"error\": logging.ERROR,\n",
    "        \"critical\": logging.CRITICAL\n",
    "    }\n",
    "\n",
    "    def __init__(self, level=\"info\", name=None,\n",
    "                 fmt=\"%(asctime)s - %(name)s[line:%(lineno)d] - %\"\n",
    "                     \"(levelname)s: %(message)s\"):\n",
    "        logging.basicConfig(level=self.level_relations.get(level), format=fmt)\n",
    "        self.logger = logging.getLogger(name)\n",
    "\n",
    "\n",
    "log = Logger(level='info', name=__name__).logger\n",
    "\n",
    "\n",
    "def get_ks(target, y_pred):\n",
    "    df = pd.DataFrame({\n",
    "        'y_pred': y_pred,\n",
    "        'target': target,\n",
    "    })\n",
    "    df = df.sort_values(by='y_pred', ascending=False)\n",
    "    df['good'] = 1 - df['target']\n",
    "    df['bad_rate'] = df['target'].cumsum() / df['target'].sum()\n",
    "    df['good_rate'] = df['good'].cumsum() / df['good'].sum()\n",
    "    df['ks'] = df['bad_rate'] - df['good_rate']\n",
    "    return max(abs(df['ks']))\n",
    "\n",
    "\n",
    "# auc\n",
    "def get_roc_auc_score(target, y_pred):\n",
    "    if target.nunique() != 2:\n",
    "        raise ValueError('the target is not 2 classier target')\n",
    "    else:\n",
    "        return roc_auc_score(target, y_pred)\n",
    "\n",
    "\n",
    "def get_splitted_data(df_selected, target, selected_features):\n",
    "    X = {}\n",
    "    y = {}\n",
    "\n",
    "    X['all'] = df_selected[selected_features]\n",
    "    y['all'] = df_selected[target]\n",
    "\n",
    "    for name, df in df_selected.groupby('type'):\n",
    "        X[name] = df[selected_features]\n",
    "        y[name] = df[target]\n",
    "\n",
    "    if not X.__contains__('oot'):\n",
    "        X['oot'] = None\n",
    "        y['oot'] = None\n",
    "\n",
    "    return X['all'], y['all'], X['train'], y['train'], X['test'], y['test'], X['oot'], y['oot']\n",
    "\n",
    "\n",
    "def to_score(x):\n",
    "    import math\n",
    "    if x <= 0.001:\n",
    "        x = 0.001\n",
    "    elif x >= 0.999:\n",
    "        x = 0.999\n",
    "\n",
    "    A = 404.65547022\n",
    "    B = 72.1347520444\n",
    "    result = int(round(A - B * math.log(x / (1 - x))))\n",
    "\n",
    "    if result < 0:\n",
    "        result = 0\n",
    "    if result > 1200:\n",
    "        result = 1200\n",
    "    result = 1200 - result\n",
    "    return result\n",
    "\n",
    "\n",
    "def psi_statis(df_src, splitted_types, scores):\n",
    "    def bin_psi(x, y):\n",
    "\n",
    "        if pd.isnull(y) or y == 0 or pd.isnull(x) or x == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return (x - y) * math.log(x / y)\n",
    "\n",
    "    if 'train' not in splitted_types:\n",
    "        print('Error: failt to get psi, for train is not in splitted_types')\n",
    "        return\n",
    "\n",
    "    bins = list(range(300, 951, 50))\n",
    "    l = []\n",
    "    for (client_batch, splitted_type), df_type in df_src.groupby(['client_batch', 'type']):\n",
    "        for score in scores:\n",
    "            df_score = df_type[df_type[score].notnull()]\n",
    "            df = pd.cut(df_score[score].map(to_score), bins=bins, right=False).value_counts().map(\n",
    "                lambda v: v / df_score.shape[0] if df_score.shape[0] > 0 else np.nan).to_frame('pct')\n",
    "            df.index.name = 'bin'\n",
    "            df.index = df.index.astype(str)\n",
    "            df = df.reset_index()\n",
    "\n",
    "            df['client_batch'] = client_batch\n",
    "            df['type'] = splitted_type\n",
    "            df['feature'] = score\n",
    "\n",
    "            l.append(df)\n",
    "\n",
    "    df_psi_detail = pd.concat(l, ignore_index=True).pivot_table(index=['client_batch', 'feature', 'bin'],\n",
    "                                                                columns='type', values='pct')\n",
    "    df_psi_detail.columns = [s + '_pct' for s in df_psi_detail.columns.format()]\n",
    "    df_psi_detail = df_psi_detail.reset_index()\n",
    "\n",
    "    for splitted_type in filter(lambda s: s != 'train', splitted_types):\n",
    "        df_psi_detail['train_{}_psi'.format(splitted_type)] = df_psi_detail.apply(\n",
    "            lambda r: bin_psi(r['train_pct'], r[splitted_type + '_pct']), axis=1)\n",
    "\n",
    "    psi_col = list(filter(lambda col: '_psi' in col, df_psi_detail.columns.format()))\n",
    "    df_psi = df_psi_detail.groupby(['client_batch', 'feature']).sum()[psi_col].reset_index()\n",
    "\n",
    "    df_psi_detail_sum = df_psi_detail.drop(labels='bin', axis=1).groupby(\n",
    "        ['client_batch', 'feature']).sum().reset_index()\n",
    "    df_psi_detail_sum['bin'] = '[sum]'\n",
    "\n",
    "    df_psi_detail = pd.concat([df_psi_detail, df_psi_detail_sum], ignore_index=True).sort_values(\n",
    "        ['client_batch', 'feature'])\n",
    "    df_psi_detail = pd.DataFrame(df_psi_detail, columns=['client_batch', 'feature', 'bin',\n",
    "                                                         'train_pct', 'test_pct', 'oot_pct', 'train_test_psi',\n",
    "                                                         'train_oot_psi'])\n",
    "\n",
    "    return df_psi, df_psi_detail\n",
    "\n",
    "\n",
    "def train_test_split_(df_src, target='y_target', test_size=0.3):\n",
    "    \"\"\"\n",
    "    样本切分函数.先按target分类，每类单独切成train/test，再按train/test合并，\n",
    "    使得train/test的badrate能高度一致\n",
    "    :param df_src:\n",
    "    :param target:\n",
    "    :param test_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    l = [[], [], [], []]\n",
    "    for target_value, X in df_src.groupby(target):\n",
    "\n",
    "        X[target] = target_value\n",
    "\n",
    "        row = train_test_split(X.drop(labels=target, axis=1), X[target], test_size=test_size, random_state=1234)\n",
    "\n",
    "        for i in range(0, 4):\n",
    "            l[i].append(row[i])\n",
    "\n",
    "    list_df = []\n",
    "    for i in range(0, 4):\n",
    "        list_df.append(pd.concat(l[i]))\n",
    "\n",
    "    return tuple(list_df)\n",
    "\n",
    "\n",
    "def split_data_type(df, key_col='tdid', target='target', apply_time='apply_time', test_size=0.3):\n",
    "    df_id = df.copy()\n",
    "    if df_id[target].isin([0, 1]).all():\n",
    "        print('样本y值在0，1')\n",
    "    else:\n",
    "        print('\\033[0;31m样本y值不在0，1之间，请检查！！！\\033[0m')\n",
    "\n",
    "    print('样本情况：', df_id.shape)\n",
    "    df_id.drop_duplicates(subset=key_col, inplace=True)\n",
    "    print('分布情况：', df_id.groupby(target)[key_col].count().sort_index())\n",
    "    # df_id.groupby(target)['tdid'].count().sort_index().to_excel(\n",
    "    #     '{}{}_id_distributed.xlsx'.format(data_dir, client_batch))\n",
    "    print('样本drop_duplicates情况：', df_id.shape)\n",
    "\n",
    "    df_id = df_id.loc[df_id[target].isin([0, 1])]\n",
    "    print('样本y值在0，1的样本情况：', df_id.shape)\n",
    "\n",
    "    # ---------查看各月badrate---------------------\n",
    "    df_id['apply_month'] = df_id[apply_time].map(lambda s: s[:7])\n",
    "    print(df_id.groupby('apply_month').describe()[target])\n",
    "\n",
    "    # ---------样本划分----------------------------\n",
    "    ##需要oot\n",
    "    # df_selected = df_id #can filter records here\n",
    "    # # df_oot = df_selected[df_selected['apply_time']>= '2019-04-01']\n",
    "    # # X_train = df_selected[df_selected['apply_time']<= '2019-01-31']\n",
    "    # # X_test = df_selected[(df_selected['apply_time']> '2019-01-31') & (df_selected['apply_time']< '2019-04-01')]\n",
    "\n",
    "    # df_oot = df_selected[df_selected['apply_time']>= '2019-03-01']\n",
    "    # X_train = df_selected[df_selected['apply_time']<= '2018-12-31']\n",
    "    # X_test = df_selected[(df_selected['apply_time']> '2018-12-31') & (df_selected['apply_time']< '2019-03-01')]\n",
    "\n",
    "    # #X_train, X_test, y_train, y_test = geo_train_test_split(df_not_oot,label=label)\n",
    "\n",
    "    # df_id.loc[df_oot.index,'type'] = 'oot'\n",
    "    ##需要oot\n",
    "\n",
    "    # 不需要oot的时候运行下面这一行代码\n",
    "    X_train, X_test, y_train, y_test = train_test_split_(df_id, target=target, test_size=test_size)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(df_id.drop(columns=target), df_id[target], test_size=test_size,\n",
    "    #                                                     random_state=123)\n",
    "    # 不需要oot的时候运行下面这一行代码\n",
    "\n",
    "    df_id.loc[X_train.index, 'type'] = 'train'\n",
    "    df_id.loc[X_test.index, 'type'] = 'test'\n",
    "\n",
    "    print(df_id.groupby('type').describe()[target])\n",
    "\n",
    "    # ----------输出---------------------------------\n",
    "    # df_id.to_csv(data_dir + '{}_split.csv'.format(client_batch), index=False)\n",
    "    return df_id\n",
    "\n",
    "\n",
    "def select_features_dtypes(df, exclude=None):\n",
    "    '''\n",
    "    根据数据集，筛选出数据类型\n",
    "    :param df: 数据集\n",
    "    :param exclude: 排除不需要参与筛选的列\n",
    "    :return:三个list\n",
    "    '''\n",
    "    if exclude is not None:\n",
    "        df = df.drop(columns=exclude)\n",
    "    # 筛选出数值类型列\n",
    "    numeric_df = df.select_dtypes([np.number])\n",
    "\n",
    "    no_numeric_df = df.select_dtypes(include=['object'])\n",
    "    # 将object类型的列尝试转成时间类型\n",
    "    dates_objs_df = no_numeric_df.apply(pd.to_datetime, errors='ignore')\n",
    "    # 筛选出字符类型列\n",
    "    objs_df = dates_objs_df.select_dtypes(include=['object'])\n",
    "    # 筛选出时间类型列\n",
    "    dates_df = list(set(dates_objs_df.columns) - set(objs_df.columns))\n",
    "\n",
    "    assert len(numeric_df.columns) + len(objs_df.columns) + len(dates_df) == df.shape[1]\n",
    "\n",
    "    return numeric_df.columns.tolist(), objs_df.columns.tolist(), dates_df\n",
    "\n",
    "\n",
    "def category_2_woe(df, category_cols=[], target='target'):\n",
    "    '''\n",
    "    方法说明。每个类别都会转成woe值。缺失值不转，即还是为缺失值。在考虑到未来如果有新类别，给予other对应woe为0\n",
    "    :param df:\n",
    "    :param category_cols:\n",
    "    :param target:\n",
    "    :return:\n",
    "    '''\n",
    "    var_value_woe = {}\n",
    "    for i in category_cols:\n",
    "        # bin_g = df.groupby(by=i)[target].agg({'total_cnt': 'count', 'bad_cnt': 'sum'})\n",
    "        # https://stackoverflow.com/questions/60229375/solution-for-specificationerror-nested-renamer-is-not-supported-while-agg-alo\n",
    "        bin_g = df.groupby(by=i)[target].agg([('total_cnt', 'count'), ('bad_cnt', 'sum')])\n",
    "        bin_g['good_cnt'] = bin_g['total_cnt'] - bin_g['bad_cnt']\n",
    "        bin_g['bad_rate'] = bin_g['bad_cnt'] / sum(bin_g['bad_cnt'])\n",
    "        bin_g['good_rate'] = bin_g['good_cnt'] / sum(bin_g['good_cnt'])\n",
    "        bin_g['good_rate'].replace({0: 0.0000000001}, inplace=True)  # good_rate为0的情况下，woe算出来是-inf。即将0使用一个极小数替换\n",
    "        bin_g['woe'] = bin_g.apply(lambda x: 0.0 if x['bad_rate'] == 0 else np.log(x['good_rate'] / x['bad_rate']),\n",
    "                                   axis=1)\n",
    "\n",
    "        value_woe = bin_g['woe'].to_dict()\n",
    "        value_woe['other'] = 0  # 未来有新类别的情况下，woe值给予0\n",
    "        var_value_woe[i] = value_woe\n",
    "\n",
    "    return var_value_woe\n",
    "\n",
    "\n",
    "def category_2_woe_save(var_value_woe, path=None):\n",
    "    if path is None:\n",
    "        path = sys.path[0]\n",
    "\n",
    "    with open(path + 'category_var_value_woe.json', 'w') as f:\n",
    "        json.dump(var_value_woe, f)\n",
    "\n",
    "\n",
    "def category_2_woe_load(path=None):\n",
    "    with open(path + 'category_var_value_woe.json', 'r') as f:\n",
    "        var_value_woe = json.load(f)\n",
    "    return var_value_woe\n",
    "\n",
    "\n",
    "def filter_miss(df, miss_threshold=0.9):\n",
    "    '''\n",
    "\n",
    "    :param df: 数据集\n",
    "    :param miss_threshold: 缺失率大于等于该阈值的变量剔除\n",
    "    :return:\n",
    "    '''\n",
    "    names_list = []\n",
    "    for name, series in df.items():\n",
    "        n = series.isnull().sum()\n",
    "        miss_q = n / series.size\n",
    "        if miss_q < miss_threshold:\n",
    "            names_list.append(name)\n",
    "    return names_list\n",
    "\n",
    "\n",
    "# =============================\n",
    "\n",
    "class WoeTransformer(TransformerMixin):\n",
    "\n",
    "    def __init__(self, n_jobs=2):\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def bin_to_woe(self, df, var_bin_woe_dict):\n",
    "        '''\n",
    "        根据传进来的var_bin_woe_dict对原始值进行映射。\n",
    "        如在var_bin_woe_dict没有的类别（数据集中新出现的类别，归为到other这类）同时var_bin_woe_dict中得有other该类别对应的woe值\n",
    "        如果var_bin_woe_dict中没有other该类别对应的woe值，即数据集中新出现的类别归为缺失值，即新出现的类别没有woe值\n",
    "        :param df:\n",
    "        :param var_bin_woe_dict:    形如{\"Sex\": {\"female\": -1.5298770033401874, \"male\": 0.9838327092415774}, \"Embarked\": {\"C\": -0.694264203516269, \"S\": 0.1977338357888416, \"other\": -0.030202603851420356}}\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        for feature, bin_woe in var_bin_woe_dict.items():\n",
    "            df[feature] = df[feature].map(\n",
    "                lambda x: x if (x in bin_woe.keys() or x is np.nan or pd.isna(x)) else 'other')\n",
    "            df[feature] = df[feature].map(bin_woe)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, df, var_bin_woe_dict, bins_dict={}):\n",
    "        '''\n",
    "        输入三列的dataframe，['var_name','range','woe'] 返回转换woe后的数据集\n",
    "        :param var_bin_woe_dict:    形如{\"Sex\": {\"female\": -1.5298770033401874, \"male\": 0.9838327092415774}, \"Embarked\": {\"C\": -0.694264203516269, \"S\": 0.1977338357888416, \"other\": -0.030202603851420356}}\n",
    "        :return:转换woe后的数据集\n",
    "        '''\n",
    "\n",
    "        df_ = df.copy()\n",
    "        if bins_dict:\n",
    "            print('需要将原始数据转bin')\n",
    "            df_ = self.data_to_bin(df, bins_dict=bins_dict)\n",
    "        return self.bin_to_woe(df_, var_bin_woe_dict)\n",
    "\n",
    "    def data_to_bin(self, df, bins_dict={}):\n",
    "        '''\n",
    "        原始数据根据bins_dict进行分箱\n",
    "        :param df:含有目标变量的数据集；不需要返回var_summary可以不需要目标变量，将函数中target部分注释\n",
    "        :param target:目标值变量名称\n",
    "        :param bins_dict:分箱字典, 形如{'D157': [-999, 1.0, 2.0, 3.0, 5.0, inf]}\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        if not isinstance(bins_dict, dict):\n",
    "            assert '请传入类似 {\\'D157\\': [-999, 1.0, 2.0, 3.0, 5.0, inf]}'\n",
    "\n",
    "        data_with_bins = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(pd.cut)(df[col], bins=bins, right=False, retbins=True) for col, bins in bins_dict.items())\n",
    "        data_bin = pd.DataFrame([i[0].astype(str) for i in data_with_bins]).T\n",
    "        b_dict = dict([(i[0].name, i[1].tolist()) for i in data_with_bins])\n",
    "        if not operator.eq(bins_dict, b_dict):\n",
    "            assert '传入的分箱和应用后的分箱不对等，请联系开发者'\n",
    "\n",
    "        return data_bin\n",
    "\n",
    "\n",
    "# =============================\n",
    "\n",
    "class ModelTune():\n",
    "    def __init__(self):\n",
    "        self.base_model = None\n",
    "        self.best_model = None\n",
    "        self.model_params = None\n",
    "        self.loss = np.inf\n",
    "        self.metrics = None\n",
    "        self.default_params = None\n",
    "        self.int_params = None\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.best_model\n",
    "\n",
    "    def fit(self, train_data=(), test_data=()\n",
    "            , init_points=10, iterations=15):\n",
    "\n",
    "        X_train, y_train = train_data\n",
    "        X_test, y_test = test_data\n",
    "\n",
    "        # def loss_fun(train_result, test_result):\n",
    "        #     train_result = train_result * 100\n",
    "        #     test_result = test_result * 100\n",
    "        #     if train_result == test_result:\n",
    "        #         return test_result\n",
    "        #\n",
    "        #     import math\n",
    "        #     return test_result - math.log(abs(test_result - train_result))\n",
    "\n",
    "        def loss_fun(train_result, test_result):\n",
    "            train_result = train_result * 100\n",
    "            test_result = test_result * 100\n",
    "\n",
    "            return test_result - 2 ** abs(test_result - train_result)\n",
    "\n",
    "        # def loss_fun(train_result, test_result):\n",
    "        #     train_result = train_result * 100\n",
    "        #     test_result = test_result * 100\n",
    "        #\n",
    "        #     return train_result - 2 ** abs(train_result - test_result)\n",
    "\n",
    "        def obj_fun(**params):\n",
    "            for param in self.int_params:\n",
    "                params[param] = int(round(params[param]))\n",
    "\n",
    "            model = self.base_model(**params, **self.default_params)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            pred_test = model.predict_proba(X_test)[:, 1]\n",
    "            pred_train = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "            test_auc = get_roc_auc_score(y_test, pred_test)\n",
    "            train_auc = get_roc_auc_score(y_train, pred_train)\n",
    "            print('test_auc is : ', test_auc)\n",
    "            print('train_auc is : ', train_auc)\n",
    "\n",
    "            test_ks = get_ks(y_test, pred_test)\n",
    "            train_ks = get_ks(y_train, pred_train)\n",
    "\n",
    "            max_result = loss_fun(train_auc, test_auc)\n",
    "            # max_result = loss_fun(train_ks, test_ks) * 2 + loss_fun(train_auc, test_auc)\n",
    "\n",
    "            loss = 1 - max_result\n",
    "            if loss < self.loss:\n",
    "                self.loss = loss\n",
    "                self.best_model = model\n",
    "                print('best model result is {}'.format(1 - loss))\n",
    "                print('best model result is : ')\n",
    "                print(self.best_model.get_params())\n",
    "            print('current obj_fun result is : ', max_result)\n",
    "\n",
    "            return max_result\n",
    "\n",
    "        params_optimizer = BayesianOptimization(obj_fun, self.model_params, random_state=1)\n",
    "        print('params_optimizer is : ', params_optimizer.space.keys)\n",
    "\n",
    "        print('begain optimizer params!!!')\n",
    "        start = time.time()\n",
    "        params_optimizer.maximize(init_points=init_points, n_iter=iterations, acq='ei', xi=0.0)\n",
    "        # params_optimizer.maximize(init_points=init_points, n_iter=iterations, acq='ucb', xi=0.0, alpha=1e-6)\n",
    "        end = time.time()\n",
    "        print('optimizer params over!!! 共耗时{} 分钟'.format((end - start) / 60))\n",
    "        print('the best params is : {}'.format(params_optimizer.max['params']))\n",
    "        print('Maximum xgb value is : {}'.format(params_optimizer.max['target']))\n",
    "\n",
    "\n",
    "class ClassifierModel(ModelTune):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = ['auc', 'ks']\n",
    "\n",
    "\n",
    "class RegressorModel(ModelTune):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = ['r2', 'rmse']\n",
    "\n",
    "\n",
    "class XGBClassifierTuner(ClassifierModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 先执行父类\n",
    "\n",
    "        self.base_model = XGBClassifier\n",
    "        self.model_params = {\n",
    "            'min_child_weight': (1, 300),\n",
    "            'max_depth': (2, 10),\n",
    "            'n_estimators': (50, 300),\n",
    "            'learning_rate': (0.01, 0.2),\n",
    "            'subsample': (0.4, 1.0),\n",
    "            'colsample_bytree': (0.3, 1.0),\n",
    "            'gamma': (0, 2.0),\n",
    "            'reg_alpha': (0, 2.0),\n",
    "            'reg_lambda': (0, 2.0),\n",
    "            # 'max_delta_step': (0, 10)\n",
    "        }\n",
    "\n",
    "        self.default_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'n_jobs': -1,\n",
    "            'nthread': -1\n",
    "        }\n",
    "\n",
    "        self.int_params = ['max_depth', 'n_estimators']\n",
    "\n",
    "\n",
    "class LGBClassifierTuner(ClassifierModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 先执行父类\n",
    "\n",
    "        self.base_model = LGBMClassifier\n",
    "        self.model_params = {\n",
    "            'max_depth': (500, 1500),\n",
    "            'num_leaves': (200, 800),\n",
    "            'min_data_in_leaf': (50, 250),\n",
    "            'n_estimators': (750, 1800),\n",
    "            'min_child_weight': (0.01, 0.05),\n",
    "            'bagging_fraction': (0.2, 1.0),\n",
    "            'feature_fraction': (0.15, 1.0),\n",
    "            'learning_rate': (0.005, 0.01),\n",
    "            'reg_alpha': (0.2, 0.6),\n",
    "            'reg_lambda': (0.25, 1.0)\n",
    "        }\n",
    "\n",
    "        self.default_params = {\n",
    "            'objective': 'binary',\n",
    "            # 'max_depth': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'bagging_seed': 11,\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'random_state': 47,\n",
    "            'num_threads': -1\n",
    "        }\n",
    "\n",
    "        self.int_params = ['max_depth', 'num_leaves', 'min_data_in_leaf', 'n_estimators']\n",
    "\n",
    "\n",
    "classifiers_dic = {\n",
    "    # 'logistic_regression': LogisticRegressionTuner,\n",
    "    # 'random_forest': RandomForestClassifierTuner,\n",
    "    'xgboost': XGBClassifierTuner,\n",
    "    # 'lgb': LGBClassifierTuner\n",
    "}\n",
    "\n",
    "\n",
    "def classifiers_model(models=[], metrics=[], train_data=(), test_data=()\n",
    "                      , init_points=10, iterations=25, verbose=1):\n",
    "    if type(models) != list:\n",
    "        raise AttributeError('Argument `models` must be a list, ',\n",
    "                             'but given {}'.format(type(models)))\n",
    "    if len(models) == 0:\n",
    "        models = list(classifiers_dic.keys())\n",
    "    classifiers = []\n",
    "    for model in models:\n",
    "        if model in classifiers_dic:\n",
    "            classifiers.append(classifiers_dic[model])\n",
    "    loss = float('inf')\n",
    "    _model = None\n",
    "    for classifier in classifiers:\n",
    "        if verbose:\n",
    "            print(\"Optimizing {}...\".format(classifier()))\n",
    "        _model = classifier()\n",
    "        _model.fit(train_data=train_data,\n",
    "                   test_data=test_data\n",
    "                   , init_points=init_points, iterations=iterations)\n",
    "\n",
    "    return _model.get_model()\n",
    "\n",
    "\n",
    "def sigle_feature_fit_model(train_data=(), test_data=(), is_noise=False, is_only_return_auc=True):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    l = []\n",
    "    for i in X_train:\n",
    "        # xclf = xgb.XGBClassifier(colsample_bytree=0.3, seed=123, random_state=1234)\n",
    "        xclf = xgb.XGBClassifier(**{\n",
    "            'objective': 'binary:logistic',\n",
    "            'n_jobs': -1,\n",
    "            'nthread': -1\n",
    "        })\n",
    "        xclf.fit(X_train[[i]], y_train)\n",
    "        pred_y_train = xclf.predict_proba(X_train[[i]])[:, 1]\n",
    "        auc = get_roc_auc_score(y_train, pred_y_train)\n",
    "        ks = get_ks(y_train, pred_y_train)\n",
    "        l.append((i, auc, ks))\n",
    "\n",
    "    var_auc_ks_df = pd.DataFrame(l, columns=['features', 'auc', 'ks'])\n",
    "\n",
    "    var_auc_ks_df = var_auc_ks_df[var_auc_ks_df['auc'] > 0.51]\n",
    "    # var_auc_ks_df = var_auc_ks_df[var_auc_ks_df['auc'] > 0.1]\n",
    "    # var_auc_ks_df.sort_values(by=['auc', 'ks'], ascending=False, inplace=True)\n",
    "    # print(var_auc_ks_df)\n",
    "    if is_noise:\n",
    "        var_auc_ks_df.to_excel('xgb_not_del_corr_var_auc_ks_df.xlsx')\n",
    "    else:\n",
    "        var_auc_ks_df.to_excel('xgb_var_auc_ks_df.xlsx')\n",
    "    if is_only_return_auc:\n",
    "        return var_auc_ks_df.drop(columns='ks')\n",
    "    else:\n",
    "        return var_auc_ks_df\n",
    "\n",
    "\n",
    "def sigle_feature_auc_ks(train_data=(), test_data=(), is_noise=False, is_only_return_auc=True):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    l = []\n",
    "    for i in X_train:\n",
    "        auc = get_roc_auc_score(y_train, X_train[i])\n",
    "        ks = get_ks(y_train, X_train[i])\n",
    "        l.append((i, auc, ks))\n",
    "\n",
    "    var_auc_ks_df = pd.DataFrame(l, columns=['features', 'auc', 'ks'])\n",
    "    # var_auc_ks_df.sort_values(by=['auc', 'ks'], ascending=False, inplace=True)\n",
    "    # print(var_auc_ks_df)\n",
    "    # if is_noise:\n",
    "    #     var_auc_ks_df.to_excel('process_after_data/xgb_not_del_corr_var_auc_ks_df_zhijie.xlsx')\n",
    "    # else:\n",
    "    #     var_auc_ks_df.to_excel('process_after_data/xgb_var_auc_ks_df_zhijie.xlsx')\n",
    "    if is_only_return_auc:\n",
    "        return var_auc_ks_df.drop(columns='ks')\n",
    "    else:\n",
    "        return var_auc_ks_df\n",
    "\n",
    "\n",
    "def change_col_subsample_fit_model(train_data=(), test_data=()):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    colsample_bytree = [i / 10 for i in range(3, 11)]\n",
    "    subsample = [i / 10 for i in range(3, 11)]\n",
    "\n",
    "    imp_l = []\n",
    "    for i in range(8):\n",
    "        params = {\n",
    "            'min_child_weight': 10,\n",
    "            'subsample': subsample[i],\n",
    "            'colsample_bytree': colsample_bytree[i],\n",
    "            'objective': 'binary:logistic',\n",
    "            'n_jobs': -1,\n",
    "            'nthread': -1\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        imp = pd.DataFrame(list(model.get_booster().get_score().items()),\n",
    "                           columns=['features', 'feature_importances']).set_index('features')\n",
    "        imp_l.append(imp)\n",
    "\n",
    "        pred_y_train = model.predict_proba(X_train)[:, 1]\n",
    "        pred_y_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        print('subsample is {} and colsample_bytree is {} model result is : '.format(subsample[i], colsample_bytree[i]))\n",
    "        print('xgb train auc is ：', get_roc_auc_score(y_train, pred_y_train))\n",
    "        print('xgb train ks is ：', get_ks(y_train, pred_y_train))\n",
    "        print('xgb test auc is ：', get_roc_auc_score(y_test, pred_y_test))\n",
    "        print('xgb test ks is ：', get_ks(y_test, pred_y_test))\n",
    "\n",
    "    imp_df = pd.concat(imp_l, axis=1)\n",
    "    imp_df['mean_imp'] = imp_df.mean(axis=1)\n",
    "    # imp_df.to_excel('imp_df_all_mean.xlsx')\n",
    "    imp_df.drop(columns=['feature_importances'], inplace=True)\n",
    "    # imp_df.to_excel('imp_df_mean.xlsx')\n",
    "    imp_df.reset_index(inplace=True)\n",
    "    return imp_df\n",
    "\n",
    "\n",
    "def kfold_xgb_model(train_data=(), is_noise=False, cv=StratifiedKFold(10, shuffle=True)):\n",
    "    X, y = train_data\n",
    "    cv_data = cv.split(X, y)\n",
    "\n",
    "    train_auc_l = []\n",
    "    valid_auc_l = []\n",
    "    feature_imp = []\n",
    "    for fold_num, (train_i, valid_i) in enumerate(cv_data):\n",
    "        X_train, y_train = X.iloc[train_i], y.iloc[train_i]\n",
    "        X_valid, y_valid = X.iloc[valid_i], y.iloc[valid_i]\n",
    "\n",
    "        # print(X.shape)\n",
    "        # print(X_train.shape)\n",
    "        # print(X_valid.shape)\n",
    "\n",
    "        model = xgb.XGBClassifier(**{\n",
    "            'objective': 'binary:logistic',\n",
    "            'n_jobs': -1,\n",
    "            'nthread': -1\n",
    "        })\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        pred_y_train = model.predict_proba(X_train)[:, 1]\n",
    "        pred_y_valid = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        train_auc = get_roc_auc_score(y_train, pred_y_train)\n",
    "        valid_auc = get_roc_auc_score(y_valid, pred_y_valid)\n",
    "\n",
    "        feature_importance = pd.DataFrame(list(model.get_booster().get_score().items()),\n",
    "                                          columns=['features', 'feature_importances'])\n",
    "        # feature_importance.sort_values(by='feature_importances', ascending=False,\n",
    "        #                                inplace=True)\n",
    "        feature_importance.set_index('features', inplace=True)\n",
    "        # feature_importance.reset_index(inplace=True)\n",
    "        # print(feature_importance)\n",
    "        feature_imp.append(feature_importance)\n",
    "\n",
    "        print('Fold {} , train auc is {}, valid_auc is {}'.format(fold_num, train_auc, valid_auc))\n",
    "\n",
    "        train_auc_l.append(train_auc)\n",
    "        valid_auc_l.append(valid_auc)\n",
    "\n",
    "    train_mean_auc = np.array(train_auc_l).mean()\n",
    "    valid_mean_auc = np.array(valid_auc_l).mean()\n",
    "    feature_imp_all = pd.concat(feature_imp, axis=1)\n",
    "    print('train_mean_auc is : {}'.format(train_mean_auc))\n",
    "    print('valid_mean_auc is : {}'.format(valid_mean_auc))\n",
    "    # print('feature_imp_all is : {}'.format(feature_imp_all))\n",
    "    # feature_imp_all.to_excel('process_after_data/feature_imp_all.xlsx')\n",
    "\n",
    "    feature_imp_all['mean_imp'] = feature_imp_all.mean(axis=1)\n",
    "    # feature_imp_all.to_excel('process_after_data/feature_imp_all_mean.xlsx')\n",
    "    feature_imp_all.drop(columns='feature_importances', inplace=True)\n",
    "    # feature_imp_all.index.name = 'features'\n",
    "    feature_imp_all.reset_index(inplace=True)\n",
    "    return feature_imp_all\n",
    "\n",
    "\n",
    "def unpack_tuple(x):\n",
    "    if len(x) == 1:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def drop_corr(frame, by='auc', threshold=0.95, return_drop=False):\n",
    "    if not isinstance(by, (str, pd.Series)):\n",
    "\n",
    "        if isinstance(by, pd.DataFrame):\n",
    "            by = pd.Series(by.iloc[:, 1].values, index=by.iloc[:, 0].values)\n",
    "            # by = pd.Series(by.iloc[:, 1].values, index=frame.columns)\n",
    "        else:\n",
    "            by = pd.Series(by, index=frame.columns)\n",
    "\n",
    "    # 给重要性排下序\n",
    "    by.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    # df = frame.copy()\n",
    "\n",
    "    by.index = by.index.astype(type(frame.columns.to_list()[0]))\n",
    "    df_corr = frame[by.index.to_list()].fillna(-999).corr().abs()\n",
    "\n",
    "    ix, cn = np.where(np.triu(df_corr.values, 1) > threshold)\n",
    "\n",
    "    del_all = []\n",
    "\n",
    "    if len(ix):\n",
    "\n",
    "        for i in df_corr:\n",
    "\n",
    "            if i not in del_all:\n",
    "                # 找出与当前特征的相关性大于域值的特征\n",
    "                del_tmp = df_corr[i][(df_corr[i] > threshold) & (df_corr[i] != 1)].index.to_list()\n",
    "\n",
    "                # 比较当前特征与需要删除的特征的特征重要性\n",
    "                if del_tmp:\n",
    "                    by_tmp = by.loc[del_tmp]\n",
    "                    del_l = by_tmp[by_tmp <= by.loc[i]].index.to_list()\n",
    "                    del_all.extend(del_l)\n",
    "\n",
    "    del_f = list(set(del_all))\n",
    "\n",
    "    r = frame.drop(columns=del_f)\n",
    "\n",
    "    res = (r,)\n",
    "    if return_drop:\n",
    "        res += (del_f,)\n",
    "\n",
    "    return unpack_tuple(res)\n",
    "\n",
    "\n",
    "def forward_corr_delete(df, col_list):\n",
    "    corr_list = []\n",
    "    corr_list.append(col_list[0])\n",
    "    delete_col = []\n",
    "    # 根据特征重要性的大小进行遍历\n",
    "    for col in col_list[1:]:\n",
    "        corr_list.append(col)\n",
    "        corr = df.loc[:, corr_list].corr()\n",
    "        corr_tup = [(x, y) for x, y in zip(corr[col].index, corr[col].values)]\n",
    "        corr_value = [y for x, y in corr_tup if x != col]\n",
    "        # 若出现相关系数大于0。65，则将该特征剔除\n",
    "        if len([x for x in corr_value if abs(x) >= 0.5]) > 0:\n",
    "            delete_col.append(col)\n",
    "\n",
    "    select_corr_col = [x for x in col_list if x not in delete_col]\n",
    "    return select_corr_col\n",
    "\n",
    "\n",
    "def xgb_model(train_data=(), test_data=(), is_noise=False):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    params = {\n",
    "        'min_child_weight': 10,\n",
    "        'subsample': 0.5,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'objective': 'binary:logistic',\n",
    "        'n_jobs': -1,\n",
    "        'nthread': -1\n",
    "    }\n",
    "    xclf = xgb.XGBClassifier(**params)\n",
    "    # xclf = xgb.XGBClassifier()\n",
    "    xclf.fit(X_train, y_train)\n",
    "\n",
    "    print('===============xgb 不同的重要性===============')\n",
    "    l = []\n",
    "    print('weight')\n",
    "    importance_type = 'weight'\n",
    "    feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "                                      columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # feature_importance.set_index('features', inplace=True)\n",
    "    feature_importance.reset_index(inplace=True)\n",
    "    print(feature_importance)\n",
    "    l.append(feature_importance)\n",
    "\n",
    "    print('gain')\n",
    "    importance_type = 'gain'\n",
    "    feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "                                      columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # feature_importance.set_index('features', inplace=True)\n",
    "    feature_importance.reset_index(inplace=True)\n",
    "    # print(feature_importance)\n",
    "    l.append(feature_importance)\n",
    "\n",
    "    print('cover')\n",
    "    importance_type = 'cover'\n",
    "    feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "                                      columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # feature_importance.set_index('features', inplace=True)\n",
    "    feature_importance.reset_index(inplace=True)\n",
    "    # print(feature_importance)\n",
    "    l.append(feature_importance)\n",
    "\n",
    "    # print('total_gain')\n",
    "    # importance_type = 'total_gain'\n",
    "    # feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "    #                                   columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    # feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # # feature_importance.set_index('features', inplace=True)\n",
    "    # feature_importance.reset_index(inplace=True)\n",
    "    # print(feature_importance)\n",
    "    # l.append(feature_importance)\n",
    "    #\n",
    "    # print('total_cover')\n",
    "    # importance_type = 'total_cover'\n",
    "    # feature_importance = pd.DataFrame(list(xclf.get_booster().get_score(importance_type=importance_type).items()),\n",
    "    #                                   columns=['features', 'feature_importances_{}'.format(importance_type)])\n",
    "    # feature_importance.sort_values(by='feature_importances_{}'.format(importance_type), ascending=False, inplace=True)\n",
    "    # # feature_importance.set_index('features', inplace=True)\n",
    "    # feature_importance.reset_index(inplace=True)\n",
    "    # print(feature_importance)\n",
    "    # l.append(feature_importance)\n",
    "\n",
    "    five_importance = pd.concat(l, axis=1)\n",
    "    if is_noise:\n",
    "        five_importance.to_excel('xgb_not_del_corr_five_importance.xlsx')\n",
    "    else:\n",
    "        five_importance.to_excel('xgb_five_importance.xlsx')\n",
    "\n",
    "    print(np.mean(\n",
    "        cross_val_score(estimator=xclf, X=X_train, y=y_train, scoring='accuracy',\n",
    "                        cv=StratifiedKFold(5, random_state=123))))\n",
    "\n",
    "    pred_y_train = xclf.predict_proba(X_train)[:, 1]\n",
    "    pred_y_test = xclf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print('xgb train auc is ：', get_roc_auc_score(y_train, pred_y_train))\n",
    "    print('xgb train ks is ：', get_ks(y_train, pred_y_train))\n",
    "    print('xgb test auc is ：', get_roc_auc_score(y_test, pred_y_test))\n",
    "    print('xgb test ks is ：', get_ks(y_test, pred_y_test))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # =========================step 1 相关配置=========================\n",
    "    log.info('step 1 相关配置')\n",
    "    feature_type = 'lhpdat'  # 什么数据\n",
    "    cust_id = 'apply_no'  # 主键\n",
    "    target = 'target'  # 目标变量\n",
    "    data_type = 'type'  # 区分数据集变量\n",
    "    apply_time = 'apply_time'  # 时间\n",
    "\n",
    "    client = 'lhp09'\n",
    "    batch = 'p23'\n",
    "\n",
    "    to_model_var_num = 30  # 不限制的话修改为None\n",
    "    is_model_data_to_woe = False  # 喂入模型的数据是否需要转化为woe值，False不需要，即原始数据入模型\n",
    "    fillna_value = -999999  # 缺失值填充的值\n",
    "\n",
    "    # 阈值配置\n",
    "    exclude_cols = [apply_time, cust_id, target, data_type, 'apply_month']\n",
    "    feature_missing_threshould = 0.95  # 缺失率大于等于该阈值的变量剔除\n",
    "\n",
    "    # 需要删除的变量\n",
    "    need_drop_cols = ['applthst_loan_amount', 'tzre_report_info_report_no', 'xy_black_trade_no', 'tzre_id',\n",
    "                      'xy_black_version', 'tzre_version', 'tzre_bi_phone_number']\n",
    "\n",
    "    # 用于训练模型的数据\n",
    "    label_encoder_dict = {}\n",
    "    to_model_data_path = '/Users/ryanzheng/PycharmProjects/data_to_treemodel_v1/to_model_data/lhp_amount_rule_jm.csv'\n",
    "\n",
    "    # =========================后续代码基本可以不用动=========================\n",
    "\n",
    "    # 基本不用动\n",
    "    project_name = '{}{}'.format(client, batch)\n",
    "    client_batch = '{}{}'.format(client, batch)\n",
    "    # project_dir = 'model_result_data/{}/{}/'.format(client, batch)\n",
    "    # output_dir = '{}model/{}/'.format(project_dir, feature_type)\n",
    "    #\n",
    "    # os.makedirs(project_dir, exist_ok=True)\n",
    "    # os.makedirs(output_dir, exist_ok=True)\n",
    "    # os.makedirs(project_dir + 'data/score/', exist_ok=True)\n",
    "    # os.makedirs(project_dir + 'data/xgb_score/', exist_ok=True)\n",
    "    # 基本不用动\n",
    "    # =========================相关配置=========================\n",
    "\n",
    "    # In[38]:\n",
    "\n",
    "    # =========================step 2 读取数据集=========================\n",
    "    log.info('step 2 开始读取数据集')\n",
    "    # 读取宽表数据\n",
    "    log.info('读取样本&特征数据集：{}|{}|{}为样本数据，其他为特征数据'.format(cust_id, apply_time, target))\n",
    "    all_data = pd.read_csv(to_model_data_path)\n",
    "    all_data[target] = np.where(all_data[target] <= 200, 1, 0)\n",
    "\n",
    "    # drop_cols = ['xy_black_version', 'tzre_version']\n",
    "    # all_data.drop(columns=drop_cols, axis=1, inplace=True)\n",
    "    all_data.drop(need_drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    all_data.set_index(cust_id, inplace=True)\n",
    "    selected_features = all_data.columns.format()\n",
    "    selected_features = list(set(selected_features) - set(exclude_cols))\n",
    "    log.info('特征的个数：{}'.format(len(selected_features)))\n",
    "\n",
    "    # =========================读取字典进行重命名=========================\n",
    "    #     ##读取字典进行重命名\n",
    "    #     fea_dict_df = pd.read_excel('/home/marketingscore/ryanzheng/fit_model_project/新特征数据字典v3.xlsx')\n",
    "    #     fea_dict = fea_dict_df[['feature_code','feature_id']].set_index('feature_code')['feature_id'].to_dict()\n",
    "    #     all_data.rename(columns=fea_dict, inplace=True)\n",
    "\n",
    "    #     selected_features = all_data.columns.format()\n",
    "    #     selected_features = list(set(selected_features) - set(exclude_cols))\n",
    "    # #     if exclude_vars:\n",
    "    # #         selected_features = list(set(selected_features) - set(exclude_vars))\n",
    "\n",
    "    #     ##仅使用数据字典中有的变量\n",
    "    #     fea_dict_df_list = fea_dict_df['feature_id'].tolist()\n",
    "    #     selected_features = list(set(selected_features).intersection(set(fea_dict_df_list)))\n",
    "    #     print(len(selected_features))\n",
    "    #     ##仅使用数据字典中有的变量\n",
    "\n",
    "    # =========================读取字典进行重命名=========================\n",
    "\n",
    "    # 删除特征全为空的样本量\n",
    "    log.info('删除特征全为空的样本量')\n",
    "    print('删除特征全为空的样本之前的数据集行列：', all_data.shape)\n",
    "    all_data.dropna(subset=selected_features, how='all', inplace=True)\n",
    "    print('删除特征全为空的样本之后的数据集行列：', all_data.shape)\n",
    "\n",
    "    log.info('样本数据集情况：')\n",
    "    log.info(all_data[target].value_counts())\n",
    "    # =========================读取数据集=========================\n",
    "\n",
    "    log.info('EDA，整体数据探索性数据分析')\n",
    "    # all_data_eda = detect(all_data)\n",
    "    # all_data_eda.to_excel('{}{}_{}_all_data_eda.xlsx'.format(\n",
    "    #     output_dir, project_name, feature_type))\n",
    "\n",
    "    # =========================step 3 划分训练集和测试集=========================\n",
    "    log.info('step 3 划分训练集和测试集')\n",
    "    if data_type not in all_data.columns:\n",
    "        df_sample = all_data[[target, apply_time]]\n",
    "        df_sample.reset_index(inplace=True)\n",
    "\n",
    "        # 随机切分train、test\n",
    "        df_sample = split_data_type(df_sample, key_col=cust_id, target=target, apply_time=apply_time, test_size=0.25)\n",
    "        #df_sample.to_csv(project_dir + 'data/{}_split.csv'.format(client_batch), index=False)\n",
    "\n",
    "        #         #按时间切分\n",
    "        #         df_oot = df_sample[df_sample['apply_time']>= '2020-04-01']\n",
    "        #         X_train = df_sample[df_sample['apply_time']<= '2020-02-01']\n",
    "        #         X_test = df_sample[(df_sample['apply_time']> '2020-02-01') & (df_sample['apply_time']< '2020-04-01')]\n",
    "\n",
    "        #         df_sample.loc[df_oot.index,'type'] = 'oot'\n",
    "        #         df_sample.loc[X_train.index,'type'] = 'train'\n",
    "        #         df_sample.loc[X_test.index,'type'] = 'test'\n",
    "\n",
    "        #df_sample.to_csv(project_dir + 'data/{}_split.csv'.format(client_batch), index=False)\n",
    "        df_sample.set_index(cust_id, inplace=True)\n",
    "        print(df_sample['type'].value_counts())\n",
    "\n",
    "    # In[39]:\n",
    "\n",
    "    # 将数据集类别和数据集合并\n",
    "    # df_sample = all_data[[target, apply_time, data_type]]\n",
    "    all_data = pd.merge(df_sample[['type']], all_data, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    log.info('分开训练集和测试集为两个df')\n",
    "    train_data = all_data[all_data['type'] == 'train']\n",
    "    # test_data = all_data[all_data['type'] == 'test']\n",
    "\n",
    "    log.info('EDA，训练集探索性数据分析')\n",
    "    # detect(train_data).to_excel('{}{}_{}_train_data_eda.xlsx'.format(\n",
    "    #     output_dir, project_name, feature_type))\n",
    "    #     detect(test_data).to_excel('{}{}_{}_test_data_eda.xlsx'.format(\n",
    "    #         output_dir, project_name, feature_type))\n",
    "\n",
    "    # =========================step 4 初筛=========================\n",
    "    log.info('step 4 变量初筛')\n",
    "    # selected_features = train_data_eda[train_data_eda['missing_q'] <= 0.95].index.to_list()\n",
    "    print('删除缺失率前变量数量：', len(selected_features))\n",
    "    selected_features = filter_miss(train_data[selected_features], miss_threshold=feature_missing_threshould)\n",
    "    print('删除缺失率后变量数量：', len(selected_features))\n",
    "    train_data = train_data[selected_features + [target]]\n",
    "    # test_data = test_data[selected_features + [target]]\n",
    "    # =========================初筛=========================\n",
    "\n",
    "    # =========================step 5 数据处理=========================\n",
    "    log.info('step 5 数据woe处理')\n",
    "\n",
    "    # 离散变量数据处理\n",
    "    # selected_features = list(set(selected_features) - set(exclude_cols))\n",
    "    continuous_cols, category_cols, date_cols = select_features_dtypes(train_data[selected_features])\n",
    "\n",
    "    train_data.loc[:, continuous_cols] = train_data.loc[:, continuous_cols].fillna(fillna_value)\n",
    "    # test_data.loc[:, continuous_cols] = test_data.loc[:, continuous_cols].fillna(fillna_value)\n",
    "    all_data.loc[:, continuous_cols] = all_data.loc[:, continuous_cols].fillna(fillna_value)\n",
    "    # data.loc[:, continuous_cols] = data.loc[:, continuous_cols].fillna(-999)\n",
    "\n",
    "    # =========================labelencode=========================\n",
    "    #     def category_to_labelencoder(data, labelencoder=[]):\n",
    "    #         label_encoder_dict = {}\n",
    "    #         le = LabelEncoder()\n",
    "    #         for col in labelencoder:\n",
    "    #             print('{} in process!!!'.format(col))\n",
    "    #             data[col] = le.fit_transform(data[col].values)\n",
    "    #             number = [i for i in range(0, len(le.classes_))]\n",
    "    #             key = list(le.inverse_transform(number))\n",
    "    #             label_encoder_dict[col] = dict(zip(key, number))\n",
    "    #         return label_encoder_dict\n",
    "\n",
    "    #     def category_to_labelencoder_apply(data, labelencoder_dict={}):\n",
    "    #         for col, mapping in labelencoder_dict.items():\n",
    "    #             print('{} in process!!!'.format(col))\n",
    "    #             data[col] = data[col].map(mapping).fillna(-1)\n",
    "    #             data[col] = data[col].astype(int)\n",
    "\n",
    "    #     if category_cols:\n",
    "    #         train_data.loc[:, category_cols] = train_data.loc[:, category_cols].fillna('-1007')\n",
    "    #         all_data.loc[:, category_cols] = all_data.loc[:, category_cols].fillna('-1007')\n",
    "    #         label_encoder_dict = category_to_labelencoder(train_data, category_cols)\n",
    "    #         category_to_labelencoder_apply(all_data, label_encoder_dict)\n",
    "\n",
    "    # =========================labelencode=========================\n",
    "\n",
    "    if category_cols and not label_encoder_dict:\n",
    "        log.info('step 5.1 类别变量数据处理')\n",
    "        # train_data.loc[:, category_cols] = train_data.loc[:, category_cols].fillna('miss')\n",
    "        # test_data.loc[:, category_cols] = test_data.loc[:, category_cols].fillna('miss')\n",
    "\n",
    "        var_value_woe = category_2_woe(train_data, category_cols, target=target)\n",
    "        #category_2_woe_save(var_value_woe, '{}'.format(output_dir))\n",
    "        # var_value_woe = category_2_woe_load('{}'.format(output_dir))\n",
    "        train_data = WoeTransformer().transform(train_data, var_value_woe)\n",
    "        # test_data = WoeTransformer().transform(test_data, var_value_woe)\n",
    "        all_data = WoeTransformer().transform(all_data, var_value_woe)\n",
    "\n",
    "    # 离散变量数据处理\n",
    "\n",
    "    # In[40]:\n",
    "\n",
    "    if is_model_data_to_woe:\n",
    "        log.info('将箱子转woe')\n",
    "        log.info('============入模数据需要转化为woe值===========')\n",
    "        #         train_data_to_model = WoeTransformer().transform(train_data_bin, fb.get_var_bin_woe())\n",
    "        #         test_data_to_model = WoeTransformer().transform(test_data_bin, fb.get_var_bin_woe())\n",
    "        # all_data_to_model = WoeTransformer().transform(all_data_bin, fb.get_var_bin_woe())\n",
    "    else:\n",
    "        log.info('============入模数据不需要转化为woe值===========')\n",
    "        #         train_data_to_model = train_data.copy()\n",
    "        #         test_data_to_model = test_data.copy()\n",
    "        all_data_to_model = all_data.copy()\n",
    "\n",
    "\n",
    "    # In[41]:\n",
    "\n",
    "    def statistics_model_result(all_data=pd.DataFrame()):\n",
    "        # ===========================step 6 统计=================================\n",
    "        all_data['score'] = all_data[feature_type].map(lambda v: to_score(v))\n",
    "        log.info('模型相关结果统计！！！')\n",
    "        df_splitted_type_auc_ks = all_data.groupby(data_type).apply(\n",
    "            lambda df: pd.Series({'auc': get_roc_auc_score(df[target], df['score']),\n",
    "                                  'ks': get_ks(df[target], df['score'])}))\n",
    "        df_splitted_type_auc_ks = df_splitted_type_auc_ks.reindex(['train', 'test', 'oot', 'cv'])\n",
    "\n",
    "        log.info('模型效果：')\n",
    "        print(df_splitted_type_auc_ks)\n",
    "\n",
    "        all_data['month'] = all_data[apply_time].map(lambda s: s[:7])\n",
    "        df_monthly_auc_ks = all_data.groupby('month').apply(\n",
    "            lambda df: pd.Series({'auc': get_roc_auc_score(df[target], df['score']),\n",
    "                                  'ks': get_ks(df[target], df['score'])}))\n",
    "        del all_data['month']\n",
    "        log.info('不同月份的模型效果：')\n",
    "        print(df_monthly_auc_ks)\n",
    "\n",
    "        df_desc = all_data[[feature_type, 'score']].describe()\n",
    "        df_desc.loc['coverage'] = df_desc.loc['count'] / all_data.shape[0]\n",
    "        log.info('分数describe')\n",
    "        print(df_desc)\n",
    "\n",
    "        all_data[data_type] = all_data[data_type].map(lambda s: s.lower())\n",
    "        all_data['client_batch'] = client_batch\n",
    "        # df_psi,df_psi_details = psi_statis(all_data, splitted_types=['train','test','oot'], scores=[feature_type])\n",
    "        df_psi, df_psi_details = psi_statis(all_data, splitted_types=['train', 'test'], scores=[feature_type])\n",
    "        del all_data['client_batch']\n",
    "        log.info('模型psi：')\n",
    "        print(df_psi[['train_test_psi']])\n",
    "        # log.info(df_psi[['train_test_psi','train_oot_psi']])\n",
    "\n",
    "        df_output_statis = df_splitted_type_auc_ks.reset_index()\n",
    "        df_output_statis['feature'] = feature_type\n",
    "        df_output_statis['project_name'] = project_name\n",
    "        df_output_statis['client_batch'] = client_batch\n",
    "        df_output_statis = df_output_statis.pivot_table(\n",
    "            index=['project_name', 'client_batch', 'feature'],\n",
    "            columns=data_type,\n",
    "            values=['auc', 'ks'])\n",
    "        df_output_statis.columns = ['_'.join(reversed(x)) for x in df_output_statis.columns]\n",
    "        df_output_statis['feature_cnt'] = len(selected_features)\n",
    "        df_output_statis['n_estimators'] = model.get_params()['n_estimators']\n",
    "\n",
    "        log.info('统计结束')\n",
    "        return df_output_statis\n",
    "        # ===========================统计=================================\n",
    "\n",
    "\n",
    "    # In[42]:\n",
    "\n",
    "    # =========================step 6 训练模型=========================\n",
    "    X_all, y_all, X_train, y_train, X_test, y_test, X_oot, y_oot = get_splitted_data(\n",
    "        all_data_to_model, target=target, selected_features=selected_features)\n",
    "\n",
    "    print('整体数据集大小：', X_all.shape)\n",
    "    print('训练集大小：', X_train.shape)\n",
    "    print('测试集大小：', X_test.shape)\n",
    "    if X_oot is None:\n",
    "        print('无oot数据集')\n",
    "    else:\n",
    "        print('oot集大小：', X_oot.shape)\n",
    "\n",
    "    # pd.Series(X_test.index).to_csv('{}{}_{}_X_test_key_{}.csv'.format(\n",
    "    #     output_dir, project_name, feature_type, cust_id), header=cust_id, index=False)\n",
    "\n",
    "    log.info('step 6 开始训练模型')\n",
    "    start = datetime.now()\n",
    "\n",
    "    log.info('step 6.1 ===筛选变量===')\n",
    "\n",
    "    # ===========================================\n",
    "\n",
    "    log.info('step 6.1 ===筛选变量===10折交叉后，计算变量的平均重要性')\n",
    "    # feature_imp = tree_selection.kfold_xgb_model(train_data=(del_corr_df, y_train))\n",
    "    log.info('筛选前数据集大小：{}'.format(X_train.shape))\n",
    "    feature_imp = change_col_subsample_fit_model(train_data=(X_train, y_train),\n",
    "                                                 test_data=(X_test, y_test))\n",
    "\n",
    "    log.info('将特征重要性持久化')\n",
    "    # feature_imp.to_csv('{}{}_{}_xgb_allfeature_mean_imp_df.csv'.format(\n",
    "    #     output_dir, project_name, feature_type))\n",
    "\n",
    "    log.info('根据10折拟合模型处理后的变量重要性进行变量相关性筛选')\n",
    "    del_corr_df = drop_corr(X_train, by=feature_imp, threshold=0.9)\n",
    "    # del_corr_df = tree_selection.drop_corr(del_corr_df, by=feature_imp, threshold=0.8)\n",
    "    log.info('筛选后数据集大小：{}'.format(del_corr_df.shape))\n",
    "\n",
    "    # ===========================================\n",
    "\n",
    "    selected_features = list(del_corr_df.columns)\n",
    "    log.info('最终入模变量的数量：{}'.format(len(selected_features)))\n",
    "    log.info('最终入模变量：{}'.format(selected_features))\n",
    "\n",
    "    feature_imp = change_col_subsample_fit_model(train_data=(del_corr_df, y_train),\n",
    "                                                 test_data=(X_test[del_corr_df.columns], y_test))\n",
    "\n",
    "    log.info('将待入模特征重要性持久化')\n",
    "    # feature_imp.to_csv('{}{}_{}_xgb_tomodel_feature_mean_imp_df.csv'.format(\n",
    "    #     output_dir, project_name, feature_type))\n",
    "\n",
    "    log.info('贝叶斯进行模型调参')\n",
    "    model = classifiers_model(train_data=(X_train[selected_features], y_train),\n",
    "                              test_data=(X_test[selected_features], y_test),\n",
    "                              init_points=5, iterations=8, verbose=1)\n",
    "    log.info('模型调参完成！！！')\n",
    "    log.info('模型参数：{}'.format(model.get_xgb_params()))\n",
    "    log.info('模型参数：{}'.format(model.get_params()))\n",
    "\n",
    "    df_featurescore = pd.DataFrame(list(model._Booster.get_fscore().items()), columns=['特征名称', '特征权重值']\n",
    "                                   ).sort_values('特征权重值', ascending=False)\n",
    "    # df_featurescore.to_csv('{}{}_{}_xgb_featurescore_first.csv'.format(\n",
    "    #     output_dir, project_name, feature_type), index=False)\n",
    "\n",
    "    end = datetime.now()\n",
    "    log.info('模型训练完成, 使用 {} 秒'.format((end - start).seconds))\n",
    "\n",
    "    # X_all = pd.concat([X_train, X_test])\n",
    "    X_all[feature_type] = model.predict_proba(X_all[selected_features])[:, 1]\n",
    "    all_data = pd.concat([all_data_to_model, X_all[feature_type]], axis=1)\n",
    "\n",
    "    statistics_model_result(all_data=all_data)\n",
    "\n",
    "    # X_all.to_csv('{}{}_{}_X_all.csv'.format(output_dir, project_name, feature_type))\n",
    "    # all_data.to_csv('{}{}_{}_all_data.csv'.format(output_dir, project_name, feature_type))\n",
    "\n",
    "    if to_model_var_num:\n",
    "        start = datetime.now()\n",
    "\n",
    "        print('过滤前{}个特征出来，再次训练'.format(to_model_var_num))\n",
    "        #         importance = model._Booster.get_fscore()\n",
    "        #         importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        #         features_importance = pd.DataFrame()\n",
    "        #         features_importance = features_importance.append(importance, ignore_index=True)\n",
    "        #         features_importance.columns = ['特征名称', '特征权重值']\n",
    "        #         # features_importance.to_csv(\n",
    "        #         #     '{}{}_{}_xgb_features_importance.csv'.format(output_dir, project_name, feature_type))\n",
    "        #         selected_features = features_importance.iloc[:to_model_var_num]['特征名称'].tolist()\n",
    "\n",
    "        selected_features = df_featurescore.iloc[:to_model_var_num]['特征名称'].tolist()\n",
    "\n",
    "        print('过滤后的特征：', selected_features)\n",
    "\n",
    "        X_all, y_all, X_train, y_train, X_test, y_test, X_oot, y_oot = get_splitted_data(\n",
    "            all_data_to_model, target=target, selected_features=selected_features)\n",
    "        print('整体数据集大小：', X_all.shape)\n",
    "        print('训练集大小：', X_train.shape)\n",
    "        print('测试集大小：', X_test.shape)\n",
    "        if X_oot is None:\n",
    "            print('无oot数据集')\n",
    "        else:\n",
    "            print('oot集大小：', X_oot.shape)\n",
    "\n",
    "        # 手动指定调参\n",
    "        # model = xgb.XGBClassifier(**ini_params)\n",
    "        # model.fit(X_train, y_train)\n",
    "\n",
    "        # 贝叶斯调参\n",
    "        log.info('贝叶斯进行模型调参')\n",
    "        model = classifiers_model(train_data=(X_train[selected_features], y_train),\n",
    "                                  test_data=(X_test[selected_features], y_test),\n",
    "                                  init_points=5, iterations=8, verbose=1)\n",
    "        log.info('模型调参完成！！！')\n",
    "        log.info('模型参数：{}'.format(model.get_xgb_params()))\n",
    "        log.info('模型参数：{}'.format(model.get_params()))\n",
    "\n",
    "        end = datetime.now()\n",
    "        log.info('模型训练完成, 使用 {} 秒'.format((end - start).seconds))\n",
    "\n",
    "    # X_all = pd.concat([X_train, X_test])\n",
    "    X_all[feature_type] = model.predict_proba(X_all[selected_features])[:, 1]\n",
    "    all_data = pd.concat([all_data_to_model, X_all[feature_type]], axis=1)\n",
    "\n",
    "    df_output_statis = statistics_model_result(all_data=all_data)\n",
    "\n",
    "    # X_all.to_csv('{}{}_{}_X_all.csv'.format(output_dir, project_name, feature_type))\n",
    "    # all_data.to_csv('{}{}_{}_all_data.csv'.format(output_dir, project_name, feature_type))\n",
    "\n",
    "    # ==========================训练模型=========================\n",
    "\n",
    "    # In[43]:\n",
    "\n",
    "    # ===========================step 7 模型持久化=================================\n",
    "\n",
    "    log.info('模型相关结果持久化')\n",
    "    # all_data[feature_type].to_frame().to_csv(\n",
    "    #     '{}/data/score/{}_{}_score.csv'.format(project_dir, project_name, feature_type))\n",
    "    # all_data[feature_type].to_frame().to_csv(\n",
    "    #     '{}/data/xgb_score/{}_{}_score.csv'.format(project_dir, project_name, feature_type))\n",
    "    # all_data[feature_type].to_frame().to_csv('{}{}_{}_score.csv'.format(\n",
    "    #     output_dir, project_name, feature_type))\n",
    "    #\n",
    "    # joblib.dump(model._Booster, '{}{}_{}_xgb.ml'.format(\n",
    "    #     output_dir, project_name, feature_type))\n",
    "    # json.dump(model.get_params(), open('{}{}_{}_xgb.params'.format(\n",
    "    #     output_dir, project_name, feature_type), 'w'))\n",
    "    #\n",
    "    # model._Booster.dump_model('{}{}_{}_xgb.txt'.format(output_dir, project_name, feature_type))\n",
    "    #\n",
    "    # df_featurescore = pd.DataFrame(list(model._Booster.get_fscore().items()), columns=['特征名称', '特征权重值']\n",
    "    #                                ).sort_values('特征权重值', ascending=False)\n",
    "    # df_featurescore.to_csv('{}{}_{}_xgb_featurescore.csv'.format(\n",
    "    #     output_dir, project_name, feature_type), index=False)\n",
    "    #\n",
    "    # df_corr = X_all.corr()\n",
    "    # df_corr.to_csv('{}{}_{}_xgb_corr.csv'.format(\n",
    "    #     output_dir, project_name, feature_type), index_label='feature')\n",
    "    #\n",
    "    # df_rawdata = all_data[selected_features]\n",
    "    # df_rawdata.reset_index(inplace=True)\n",
    "    # df_rawdata_col_name = df_rawdata.columns.tolist()\n",
    "    # df_rawdata_col_name.insert(len(df_rawdata_col_name) - 1,\n",
    "    #                            df_rawdata_col_name.pop(df_rawdata_col_name.index(cust_id)))\n",
    "    # df_rawdata = df_rawdata[df_rawdata_col_name]\n",
    "    # df_rawdata.head(100).to_csv('{}{}_{}_xgb_rawdata.csv'.format(\n",
    "    #     output_dir, project_name, feature_type), index=False)\n",
    "    #\n",
    "    # df_output_statis.to_csv('{}{}_{}_xgb_output_statis.csv'.format(\n",
    "    #     output_dir, project_name, feature_type))\n",
    "    #\n",
    "    # os.makedirs(project_dir + 'data/statis/auc_ks', exist_ok=True)\n",
    "    # df_output_statis.to_csv('{}data/statis/auc_ks/{}.csv'.format(\n",
    "    #     project_dir, feature_type))\n",
    "\n",
    "\n",
    "    from model_evaluator import model_save as ms\n",
    "    save_pythonmodel('td06p1_xgb_model_v1.ml',model,script_id=script_id)\n",
    "\n",
    "    log.info('模型相关结果持久化完成')\n",
    "    # ===========================模型持久化=================================\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "    # In[ ]:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
